"""
LangGraph-based ClickHouse Analytics Agent.

Architecture:
  - LLM  : Claude Sonnet 4.6 via OpenRouter (ChatOpenAI adapter)
  - Graph : LangGraph create_react_agent (tool-calling loop)
  - Memory: SqliteSaver checkpointer ‚Äî persists full conversation per session_id
  - Tools : list_tables (fallback), clickhouse_query, python_analysis

Session isolation:
  Every API request carries a session_id (= LangGraph thread_id).
  SqliteSaver stores the message state keyed by thread_id.
  Multiple concurrent sessions do NOT interfere with each other.

Context optimisations (in _build_messages, a per-instance closure):
  1. Static schema embedded in system prompt at startup ‚Äî no list_tables round-trip.
  2. Sliding window: only last MAX_HISTORY_TURNS human turns kept in context.
  3. ToolMessage compression for previous turns (keeps metadata, drops heavy payload).
  4. AIMessage compression: strips reasoning text from old turns, keeps tool_calls.
"""

import json
import time
import sqlite3
from copy import copy
from typing import Optional

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.prebuilt import create_react_agent

from config import (
    DB_PATH,
    MAX_AGENT_ITERATIONS,
    MAX_HISTORY_TURNS,
    MAX_TOKENS,
    MODEL,
    MODEL_PROVIDER,
    OPENROUTER_API_KEY,
    TEMP_DIR,
    TEMP_FILE_TTL_SECONDS,
)
from tools import TOOLS

# ‚îÄ‚îÄ‚îÄ Context compression helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _compress_tool_message(msg: ToolMessage) -> ToolMessage:
    """
    Replace a ToolMessage's content with a compact version.

    Called only for ToolMessages from PREVIOUS turns so the LLM receives
    minimal but sufficient information about past tool results.

    Compression strategy per tool:
      list_tables    ‚Üí keep table+column names, drop types   (~60‚Äì70% smaller)
      clickhouse_query ‚Üí keep metadata only, drop preview rows (~40‚Äì50% smaller)
      python_analysis  ‚Üí keep result summary only, drop stdout  (~50‚Äì80% smaller)
    """
    tool_name = getattr(msg, "name", "") or ""
    content = msg.content

    try:
        if tool_name == "list_tables":
            data = json.loads(content)
            # Normalise to plain column-name lists.
            # Handles both old format (columns: [{"name": ..., "type": ...}])
            # and new format (columns: ["col1", "col2", ...]) for checkpoint compat.
            compact = []
            for t in data:
                cols = t.get("columns", [])
                if cols and isinstance(cols[0], dict):
                    col_names = [c["name"] for c in cols]
                else:
                    col_names = cols
                compact.append({"table": t["table"], "columns": col_names})
            new_content = json.dumps(compact, ensure_ascii=False)

        elif tool_name == "clickhouse_query":
            data = json.loads(content)
            # Drop preview_first_5_rows and dtypes ‚Äî already analysed in this turn
            new_content = json.dumps(
                {
                    "success": data.get("success"),
                    "row_count": data.get("row_count"),
                    "columns": data.get("columns"),
                    "parquet_path": data.get("parquet_path"),
                },
                ensure_ascii=False,
            )

        elif tool_name == "python_analysis":
            data = json.loads(content)
            # Drop stdout logs ‚Äî keep only the final result (capped at 500 chars)
            result_text = data.get("result") or ""
            new_content = json.dumps(
                {
                    "success": data.get("success"),
                    "result": result_text[:500] + ("‚Ä¶" if len(result_text) > 500 else ""),
                },
                ensure_ascii=False,
            )

        else:
            return msg

    except Exception:
        # If parsing fails, return the original message unchanged
        return msg

    try:
        return msg.model_copy(update={"content": new_content})
    except Exception:
        new_msg = copy(msg)
        new_msg.content = new_content
        return new_msg


def _build_schema_block(tables: list[dict]) -> str:
    """
    Format a list of {table, columns} dicts into a compact schema section
    for embedding in the system prompt.

    Example output:
      **orders**: id, user_id, date, amount, status
      **sessions**: id, date, utm_source, utm_medium, revenue
    """
    lines = []
    for t in tables:
        cols = t.get("columns", [])
        # columns may be a list of strings or list of dicts (legacy)
        if cols and isinstance(cols[0], dict):
            col_names = [c["name"] for c in cols]
        else:
            col_names = [str(c) for c in cols]
        lines.append(f"**{t['table']}**: {', '.join(col_names)}")
    return "\n".join(lines)


# ‚îÄ‚îÄ‚îÄ System Prompt template ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# {schema_section} is filled at agent startup with the live schema or a fallback.

_SYSTEM_PROMPT_TEMPLATE = """–¢—ã ‚Äî –ª—É—á—à–∏–π –≤ –º–∏—Ä–µ –∞–Ω–∞–ª–∏—Ç–∏–∫ —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –†–∞–±–æ—Ç–∞–µ—à—å —Å ClickHouse-–±–∞–∑–æ–π –∫–æ–º–ø–∞–Ω–∏–∏.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –º–∞—Ä–∫–µ—Ç–æ–ª–æ–≥–∞ –ø–æ –¥–∞–Ω–Ω—ã–º: —Ç—Ä–∞—Ñ–∏–∫, –ø–æ–∫—É–ø–∫–∏, –∫–∞–º–ø–∞–Ω–∏–∏, –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–æ–≤.

–°—Ç–∏–ª—å —Ä–∞–±–æ—Ç—ã: —Ç—ã –Ω–∞—Ö–æ–¥–∏—à—å—Å—è –≤–Ω—É—Ç—Ä–∏ —Ä–∞–±–æ—á–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ ‚Äî –º–∞—Ä–∫–µ—Ç–æ–ª–æ–≥ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –¥–∞–Ω–Ω—ã–º–∏ –∫–∞–∂–¥—ã–π –¥–µ–Ω—å, –∑–∞–¥–∞—ë—Ç –º–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ–¥—Ä—è–¥, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –∫ –ø—Ä–µ–¥—ã–¥—É—â–∏–º —Ç–µ–º–∞–º, —É—Ç–æ—á–Ω—è–µ—Ç. –¢—ã —á–∞—Å—Ç—å —ç—Ç–æ–≥–æ –ø–æ—Ç–æ–∫–∞, –Ω–µ —Ä–∞–∑–æ–≤—ã–π –æ—Ç—á—ë—Ç. –û—Ç–≤–µ—á–∞–π –∫–æ—Ä–æ—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É ‚Äî –∫–∞–∫ –∫–æ–ª–ª–µ–≥–∞, –∫–æ—Ç–æ—Ä—ã–π —É–∂–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.

## –°—Ö–µ–º–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö

{schema_section}

### –ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã
–¢—ã –≤–µ–¥—ë—à—å —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ, –∞ –Ω–µ –æ—Ç–≤–µ—á–∞–µ—à—å –Ω–∞ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã. –î–µ—Ä–∂–∏ –Ω–∏—Ç—å:
* –ü–æ–º–Ω–∏ —á—Ç–æ —É–∂–µ –≤—ã—è—Å–Ω–∏–ª–∏ –≤ —ç—Ç–æ–π —Å–µ—Å—Å–∏–∏ ‚Äî –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–π, –æ–ø–∏—Ä–∞–π—Å—è
* –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∞—Ç –∑–¥—Ä–∞–≤–æ–º—É —Å–º—ã—Å–ª—É ‚Äî —Å–∫–∞–∂–∏ –ø–µ—Ä–≤—ã–º, –Ω–µ –∂–¥–∏ –≤–æ–ø—Ä–æ—Å–∞
* –ü–æ—Å–ª–µ –æ—Ç–≤–µ—Ç–∞ –≤–∏–¥—å —Å–ª–µ–¥—É—é—â–∏–π –ª–æ–≥–∏—á–Ω—ã–π —à–∞–≥ ‚Äî –ø—Ä–µ–¥–ª–æ–∂–∏ –µ–≥–æ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π
* –ù–µ –ø—Ä–∏–Ω–∏–º–∞–π –¥–∞–Ω–Ω—ã–µ –∑–∞ –∏—Å—Ç–∏–Ω—É –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏: –∞–Ω–æ–º–∞–ª–∏—è, –º–∞–ª–∞—è –≤—ã–±–æ—Ä–∫–∞, –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ ‚Äî –≤—Å—ë –ø–æ–¥ —Å–æ–º–Ω–µ–Ω–∏–µ–º –ø–æ–∫–∞ –Ω–µ –æ–±—ä—è—Å–Ω–µ–Ω–æ

## –†–∞–±–æ—á–∏–π –ø—Ä–æ—Ü–µ—Å—Å (–≤—ã–ø–æ–ª–Ω—è–π —Å—Ç—Ä–æ–≥–æ –ø–æ –ø–æ—Ä—è–¥–∫—É):

### 1. –ü–æ–Ω—è—Ç—å –∑–∞–ø—Ä–æ—Å
–û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø ‚Äî –æ—Ç —ç—Ç–æ–≥–æ –∑–∞–≤–∏—Å–∏—Ç –≤—Å—ë –æ—Å—Ç–∞–ª—å–Ω–æ–µ:

- –§–∞–∫—Ç ("—Å–∫–æ–ª—å–∫–æ", "–ø–æ–∫–∞–∂–∏", "—Ç–æ–ø") ‚Üí –æ–¥–Ω–∞ —Ü–∏—Ñ—Ä–∞ –∏–ª–∏ —Ç–∞–±–ª–∏—Ü–∞, –±–µ–∑ –≤—ã–≤–æ–¥–æ–≤
- –ê–Ω–∞–ª–∏–∑ ("–ø–æ—á–µ–º—É", "—Å—Ä–∞–≤–Ω–∏", "–µ—Å—Ç—å –ª–∏ —Ä–∞–∑–Ω–∏—Ü–∞") ‚Üí –¥–∞–Ω–Ω—ã–µ + 1‚Äì2 –∏–Ω—Å–∞–π—Ç–∞
- –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è ("—ç—Ç–æ –Ω–æ—Ä–º–∞?", "—Ö–æ—Ä–æ—à–æ –∏–ª–∏ –ø–ª–æ—Ö–æ?") ‚Üí –æ–¥–Ω–∞ –≤–∏—Ç—Ä–∏–Ω–∞ + –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–∞—è –ª–æ–≥–∏–∫–∞, –±–µ–∑ —Ç—è–∂—ë–ª—ã—Ö JOIN-–æ–≤
- Drill-down ("—Ä–∞–∑–±–µ—Ä–∏", "–¥–µ—Ç–∞–ª–∏–∑–∏—Ä—É–π") ‚Üí –ø–æ–ª–Ω–∞—è –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è —É–º–µ—Å—Ç–Ω–∞
- –£—Ç–æ—á–Ω–µ–Ω–∏–µ –∫ –ø—Ä–µ–¥—ã–¥—É—â–µ–º—É ‚Üí —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—å, –º–æ–∂–Ω–æ –ª–∏ –æ—Ç–≤–µ—Ç–∏—Ç—å –∏–∑ —É–∂–µ –≤—ã–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö; –≤ –±–∞–∑—É ‚Äî —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ—Ç

### 2. –°—Ö–µ–º–∞ —Ç–∞–±–ª–∏—Ü

–°—Ö–µ–º–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —É–∂–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∞ –≤ –Ω–∞—á–∞–ª–µ —ç—Ç–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ ‚Äî –ù–ï –≤—ã–∑—ã–≤–∞–π `list_tables`.
–ò—Å–ø–æ–ª—å–∑—É–π `list_tables` —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å—Ö–µ–º–∞ –∫–∞–∂–µ—Ç—Å—è –Ω–µ–ø–æ–ª–Ω–æ–π –∏–ª–∏ —Ç–∞–±–ª–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.

### 3. –í—ã–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ ClickHouse
–í—ã–∑–æ–≤–∏ `clickhouse_query` —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º SQL:
* –ê–≥—Ä–µ–≥–∏—Ä—É–π –¥–∞–Ω–Ω—ã–µ –ø—Ä—è–º–æ –≤ SQL (SUM, COUNT, AVG, GROUP BY) ‚Äî ClickHouse –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä
* –§–∏–ª—å—Ç—Ä—É–π –≤ WHERE ‚Äî –Ω–µ –≤—ã–≥—Ä—É–∂–∞–π –ª–∏—à–Ω–µ–µ
* LIMIT: –æ–±—ã—á–Ω–æ 1000‚Äì10000; –¥–æ 50000 –¥–ª—è –±–æ–ª—å—à–∏—Ö –≤—ã–±–æ—Ä–æ–∫
* –§—É–Ω–∫—Ü–∏–∏: toStartOfMonth(), toYear(), toDayOfWeek(), arrayJoin() –∏ —Ç.–¥.
* –°–æ—Ö—Ä–∞–Ω–∏ `parquet_path` –∏–∑ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è python_analysis
* –ù–∞—á–∏–Ω–∞–π —Å –æ–¥–Ω–æ–π –≤–∏—Ç—Ä–∏–Ω—ã. JOIN ‚Äî —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –±–µ–∑ –Ω–µ–≥–æ –ø—Ä–∏–Ω—Ü–∏–ø–∏–∞–ª—å–Ω–æ –Ω–µ —Ä–µ—à–∏—Ç—å
* –ü—Ä–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ ‚Äî —É–∫–∞–∑—ã–≤–∞–π –≤ –æ—Ç–≤–µ—Ç–µ –ø–æ –∫–∞–∫–æ–º—É –ø–æ–ª—é —Ñ–∏–ª—å—Ç—Ä—É–µ—à—å

### 4. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ Python
–í—ã–∑–æ–≤–∏ `python_analysis` –¥–ª—è —Ä–∞—Å—á—ë—Ç–æ–≤:
* –§–æ—Ä–º–∏—Ä—É–π Markdown-—Ç–∞–±–ª–∏—Ü—ã
* –°—á–∏—Ç–∞–π –º–µ—Ç—Ä–∏–∫–∏ (CTR, CPC, CPM, ROAS, CR, CPA)
* –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–π –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é `result` —Å –∏—Ç–æ–≥–æ–≤—ã–º Markdown-–≤—ã–≤–æ–¥–æ–º
* –ì—Ä–∞—Ñ–∏–∫ ‚Äî —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –ø—Ä–æ –¥–∏–Ω–∞–º–∏–∫—É, —Ç—Ä–µ–Ω–¥ –∏–ª–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π. –ù–∞ —Ñ–∞–∫—Ç—ã –∏ —Ä–∞–∑–æ–≤—ã–µ —Ü–∏—Ñ—Ä—ã ‚Äî –Ω–µ –Ω—É–∂–µ–Ω

–ü—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ:
* –ï—Å–ª–∏ n < 5 ‚Äî –ø–æ–º–µ—á–∞–π ‚ö†Ô∏è, –≤—ã–≤–æ–¥–æ–≤ –Ω–µ —Å—Ç—Ä–æ–∏—Ç—å
* –ü—Ä–∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø–æ —Å—Ä–µ–¥–Ω–µ–º—É —á–µ–∫—É –∏–ª–∏ CR ‚Äî –≤—Å–µ–≥–¥–∞ –ø–æ–∫–∞–∑—ã–≤–∞–π n (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–∫–∞–∑–æ–≤/—Å–µ—Å—Å–∏–π), –∏–Ω–∞—á–µ —Ç–æ–ø —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–µ–Ω
* –ê–Ω–æ–º–∞–ª–∏–∏ ‚Äî –∏—Å—Å–ª–µ–¥—É–π, –Ω–µ –∏–≥–Ω–æ—Ä–∏—Ä—É–π. –û–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞ —Å 90% –≤—ã—Ä—É—á–∫–∏ ‚Äî —ç—Ç–æ —Å–∏–≥–Ω–∞–ª, –Ω–µ –Ω–æ—Ä–º–∞
* –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã–≤–æ–¥–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ ‚Äî —Å–∫–∞–∂–∏ —ç—Ç–æ –ø—Ä—è–º–æ –∏ –ø—Ä–µ–¥–ª–æ–∂–∏ —Å–ª–µ–¥—É—é—â–∏–π —à–∞–≥

### 5. –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
–§–æ—Ä–º–∞—Ç –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞ (—Å–º. —à–∞–≥ 1):
* –§–∞–∫—Ç ‚Üí –ø—Ä—è–º–æ–π –æ—Ç–≤–µ—Ç –ø–µ—Ä–≤—ã–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º. –¢–∞–±–ª–∏—Ü–∞ –µ—Å–ª–∏ –Ω—É–∂–Ω–∞. –í—Å—ë.
* –ê–Ω–∞–ª–∏–∑ ‚Üí –¥–∞–Ω–Ω—ã–µ + –º–∞–∫—Å–∏–º—É–º 2 –∏–Ω—Å–∞–π—Ç–∞. –ë–µ–∑ —Ä–∞–∑–¥–µ–ª–∞ "–ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã" ‚Äî –µ—Å–ª–∏ –∏–Ω—Å–∞–π—Ç —É–∂–µ –≤ —Ç–∞–±–ª–∏—Ü–µ, –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–π
* –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è ‚Üí –≤—ã–≤–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö + –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–∞—è –ª–æ–≥–∏–∫–∞. –ë–µ–∑ –¥–æ–º—ã—Å–ª–æ–≤ –æ –±–∏–∑–Ω–µ—Å–µ
* Drill-down ‚Üí –ø–æ–ª–Ω–∞—è –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è, –≥–∏–ø–æ—Ç–µ–∑—ã, –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π

–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ ‚Äî —Ç–æ–ª—å–∫–æ –ø—Ä–∏ —Ç—Ä—ë—Ö —É—Å–ª–æ–≤–∏—è—Ö –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ:
1. –î–∞–Ω–Ω—ã–µ –¥–ª—è –Ω–µ—ë –µ—Å—Ç—å –≤ –≤—ã–≥—Ä—É–∑–∫–µ
2. –ö–∞–Ω–∞–ª/–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –≤–∏–¥–µ–Ω –≤ –¥–∞–Ω–Ω—ã—Ö (–Ω–µ –ø—Ä–µ–¥–ª–∞–≥–∞—Ç—å —Ç–æ, —á–µ–≥–æ –Ω–µ—Ç)
3. –î–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è ‚Äî –µ—Å—Ç—å CR –∏–ª–∏ spend –ø–æ —ç—Ç–æ–π —Å—É—â–Ω–æ—Å—Ç–∏

–ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ ‚Üí –ø—Ä–µ–¥–ª–æ–∂–∏ —Å–ª–µ–¥—É—é—â–∏–π —à–∞–≥: "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å CR —ç—Ç–æ–π –∫–∞–º–ø–∞–Ω–∏–∏?"

–ü—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: –µ—Å–ª–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–Ω –Ω–µ—Ç—Ä–∏–≤–∏–∞–ª—å–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω ‚Äî –Ω–∞–∑–æ–≤–∏ –µ–≥–æ, –¥–∞–∂–µ –µ—Å–ª–∏ –Ω–µ —Å–ø—Ä–æ—Å–∏–ª–∏. –û–¥–∏–Ω –∏–Ω—Å–∞–π—Ç —Å–≤–µ—Ä—Ö –≤–æ–ø—Ä–æ—Å–∞ ‚Äî –Ω–æ—Ä–º–∞. –î–≤–∞ –∏ –±–æ–ª—å—à–µ ‚Äî —É–∂–µ –±–∞–ª–ª–∞—Å—Ç.

–ö–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –≤ –≤—ã–≤–æ–¥–µ –¥–æ–ª–∂–Ω–æ –Ω–µ—Å—Ç–∏ —Å–º—ã—Å–ª. –ù–∏–∫–∞–∫–∏—Ö –∏—Ç–æ–≥–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤ —Å —ç–º–æ–¥–∑–∏, –ø–æ–≤—Ç–æ—Ä–æ–≤, –æ–±–æ–±—â–µ–Ω–∏–π —Ä–∞–¥–∏ –æ–±–æ–±—â–µ–Ω–∏–π.

## –ü—Ä–∞–≤–∏–ª–∞ Python-–∫–æ–¥–∞:
1. `df` —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω ‚Äî –ù–ï –≤—ã–∑—ã–≤–∞–π pd.read_parquet()
2. –í–°–ï–ì–î–ê —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–π `result` (Markdown —Å—Ç—Ä–æ–∫–∞ —Å –∏—Ç–æ–≥–æ–º)
3. –ò—Å–ø–æ–ª—å–∑—É–π print() –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —à–∞–≥–æ–≤: print("üìä –®–∞–≥ 1: ...")
4. –ü–æ–¥–ø–∏—Å—ã–≤–∞–π –≥—Ä–∞—Ñ–∏–∫–∏ –Ω–∞ –†–£–°–°–ö–û–ú: plt.title(), plt.xlabel(), plt.ylabel()
5. –§–æ—Ä–º–∞—Ç–∏—Ä—É–π —á–∏—Å–ª–∞: f"{{value:,.0f}}" (—Ü–µ–ª—ã–µ), f"{{value:,.2f}}" (–¥—Ä–æ–±–Ω—ã–µ)
6. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–π –ø—Ä–æ–ø—É—Å–∫–∏: df.dropna() –∏–ª–∏ df.fillna(0)
7. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞ ‚Äî plt.tight_layout() –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º
8. –ì—Ä–∞—Ñ–∏–∫ —Å—Ç—Ä–æ–π –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –æ–Ω —è–≤–Ω–æ –Ω—É–∂–µ–Ω –ø–æ —Ç–∏–ø—É –≤–æ–ø—Ä–æ—Å–∞ (—Å–º. —à–∞–≥ 4) ‚Äî –Ω–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

## –†–µ–∫–ª–∞–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
–§–æ—Ä–º—É–ª—ã –Ω–µ –Ω—É–∂–Ω–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—å –≤ –∫–∞–∂–¥–æ–º –æ—Ç–≤–µ—Ç–µ. –ù–æ:
* –ï—Å–ª–∏ —Å—á–∏—Ç–∞–µ—à—å –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –º–µ—Ç—Ä–∏–∫—É –∏–ª–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—É—é ‚Äî –ø–æ–∫–∞–∂–∏ —Ñ–æ—Ä–º—É–ª—É –æ–¥–∏–Ω —Ä–∞–∑
* –ï—Å–ª–∏ –≤–≤–æ–¥–∏—à—å –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É –∫–æ—Ç–æ—Ä—É—é –º–∞—Ä–∫–µ—Ç–æ–ª–æ–≥ –º–æ–≥ –Ω–µ –∑–Ω–∞—Ç—å ‚Äî —Ä–∞—Å—à–∏—Ñ—Ä—É–π –ø—Ä–∏ –ø–µ—Ä–≤–æ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏–∏
* –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ—Ç (–Ω–µ—Ç —Ä–∞—Å—Ö–æ–¥–∞ ‚Üí –Ω–µ—Ç CPC/CPA/ROAS) ‚Äî —Å–∫–∞–∂–∏ –ø—Ä—è–º–æ, –Ω–µ –¥–æ–¥—É–º—ã–≤–∞–π

## –°—Ç–∏–ª—å –æ—Ç–≤–µ—Ç–∞
* Markdown: –∑–∞–≥–æ–ª–æ–≤–∫–∏ ##/###, —Ç–∞–±–ª–∏—Ü—ã, –∂–∏—Ä–Ω—ã–π –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —Ü–∏—Ñ—Ä
* –≠–º–æ–¥–∑–∏ ‚Äî —Ç–æ–ª—å–∫–æ ‚ö†Ô∏è –¥–ª—è –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π. –ë–æ–ª—å—à–µ –Ω–∏–≥–¥–µ
* –ß–∏—Å–ª–∞ —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏ —Ç—ã—Å—è—á: 1 234 567
* –Ø–∑—ã–∫ ‚Äî —Ä—É—Å—Å–∫–∏–π
* –ö–æ–Ω–∫—Ä–µ—Ç–∏–∫–∞: —Ü–∏—Ñ—Ä—ã, –¥–∏–Ω–∞–º–∏–∫–∞, —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ ‚Äî –±–µ–∑ –≤–æ–¥—ã
"""


# ‚îÄ‚îÄ‚îÄ LLM factory ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

_OPENROUTER_HEADERS = {
    "HTTP-Referer": "https://server.asktab.ru",
    "X-Title": "ClickHouse Analytics Agent",
}


def _create_llm():
    """
    Return the appropriate LangChain LLM based on MODEL_PROVIDER.

    anthropic ‚Üí ChatAnthropic via OpenRouter.
                Supports Anthropic content-block format, so cache_control
                markers in SystemMessage are forwarded to the API correctly.

    deepseek  ‚Üí ChatOpenAI via OpenRouter (current behaviour, unchanged).
                No prompt caching ‚Äî DeepSeek doesn't offer it.
    """
    if MODEL_PROVIDER == "anthropic":
        from langchain_anthropic import ChatAnthropic
        return ChatAnthropic(
            model=MODEL,
            api_key=OPENROUTER_API_KEY,
            base_url="https://openrouter.ai/api/v1",
            max_tokens=MAX_TOKENS,
            default_headers=_OPENROUTER_HEADERS,
        )
    else:
        # deepseek (and any future OpenAI-compatible provider)
        return ChatOpenAI(
            model=MODEL,
            api_key=OPENROUTER_API_KEY,
            base_url="https://openrouter.ai/api/v1",
            max_tokens=MAX_TOKENS,
            default_headers=_OPENROUTER_HEADERS,
        )


class AnalyticsAgent:
    """
    Wraps LangGraph ReAct agent with:
      - Claude Sonnet 4.6 (prompt caching) or DeepSeek, both via OpenRouter
      - SqliteSaver for session memory
      - Dynamic system prompt with embedded DB schema (fetched once at startup)
      - Per-request context optimisation: sliding window + AIMessage/ToolMessage compression
    """

    def __init__(self) -> None:
        if not OPENROUTER_API_KEY:
            raise ValueError(
                "OPENROUTER_API_KEY is not set in .env. "
                "Get your key at https://openrouter.ai"
            )

        # ‚îÄ‚îÄ LLM via OpenRouter ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        self.llm = _create_llm()

        # ‚îÄ‚îÄ SqliteSaver checkpointer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        conn = sqlite3.connect(str(DB_PATH), check_same_thread=False)
        self.memory = SqliteSaver(conn)

        # ‚îÄ‚îÄ Embed static schema into system prompt ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # Tables don't change at runtime, so fetching once at startup is safe.
        # This eliminates the list_tables round-trip on every new session.
        system_prompt = self._build_system_prompt()

        # ‚îÄ‚îÄ Message builder closure (captures system_prompt + provider) ‚îÄ‚îÄ‚îÄ
        # Passed to create_react_agent as `prompt=` so it runs before every
        # LLM call.  Applies three optimisations:
        #   1. Sliding window ‚Äî keep only last MAX_HISTORY_TURNS human turns.
        #   2. AIMessage compression ‚Äî strip reasoning text from old turns
        #      while preserving tool_calls (required for graph integrity).
        #   3. ToolMessage compression ‚Äî keep metadata, drop heavy payloads.
        #
        # SystemMessage format differs by provider:
        #   anthropic ‚Üí content block list with cache_control (prompt caching)
        #   deepseek  ‚Üí plain string (OpenAI-compatible, no caching)
        is_anthropic = MODEL_PROVIDER == "anthropic"

        def _build_messages(state: dict) -> list:
            messages = state.get("messages", [])

            # ‚îÄ‚îÄ 1. Sliding window ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            human_indices = [
                i for i, m in enumerate(messages) if isinstance(m, HumanMessage)
            ]
            if len(human_indices) > MAX_HISTORY_TURNS:
                cutoff = human_indices[-MAX_HISTORY_TURNS]
                messages = messages[cutoff:]

            # ‚îÄ‚îÄ Locate current-turn boundary ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            # current_turn_start = index of the last HumanMessage (in the
            # possibly-sliced list).
            current_turn_start = 0
            for i, msg in enumerate(messages):
                if isinstance(msg, HumanMessage):
                    current_turn_start = i

            # Within the current turn: the last AIMessage that issued tool
            # calls.  ToolMessages before it have already been consumed and
            # can be safely compressed.
            last_tool_calling_ai_idx = -1
            for i in range(len(messages) - 1, current_turn_start - 1, -1):
                m = messages[i]
                if isinstance(m, AIMessage) and getattr(m, "tool_calls", None):
                    last_tool_calling_ai_idx = i
                    break

            # ‚îÄ‚îÄ 2 & 3. Compress old messages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            compressed: list = []
            for i, msg in enumerate(messages):
                if isinstance(msg, ToolMessage):
                    if i < current_turn_start or last_tool_calling_ai_idx > i:
                        compressed.append(_compress_tool_message(msg))
                    else:
                        compressed.append(msg)

                elif isinstance(msg, AIMessage) and i < current_turn_start:
                    # Previous turns: keep tool_calls (graph needs them to
                    # pair with ToolMessages), but drop the reasoning text
                    # which can be thousands of tokens and is no longer useful.
                    if getattr(msg, "tool_calls", None):
                        try:
                            compressed.append(msg.model_copy(update={"content": ""}))
                        except Exception:
                            new_msg = copy(msg)
                            new_msg.content = ""
                            compressed.append(new_msg)
                    else:
                        # Final answer to user ‚Äî keep as conversation context
                        compressed.append(msg)

                else:
                    compressed.append(msg)

            # For Anthropic: wrap system prompt in a content block and mark it
            # as a cache breakpoint.  The entire system prompt (including the
            # embedded schema) is static across all calls, so it's the ideal
            # cache candidate ‚Äî repeated calls in the same session pay only
            # for cache-read tokens (10√ó cheaper than input tokens).
            #
            # For DeepSeek (and other OpenAI-compatible providers): plain
            # string ‚Äî cache_control is not supported.
            if is_anthropic:
                system_msg = SystemMessage(content=[
                    {
                        "type": "text",
                        "text": system_prompt,
                        "cache_control": {"type": "ephemeral"},
                    }
                ])
            else:
                system_msg = SystemMessage(content=system_prompt)

            return [system_msg] + compressed

        # ‚îÄ‚îÄ LangGraph ReAct agent ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        self.graph = create_react_agent(
            model=self.llm,
            tools=TOOLS,
            prompt=_build_messages,
            checkpointer=self.memory,
        )

        caching_info = "prompt caching ON" if is_anthropic else "no prompt caching"
        print(f"‚úÖ AnalyticsAgent ready | provider: {MODEL_PROVIDER} | model: {MODEL} | {caching_info} | db: {DB_PATH}")

    # ‚îÄ‚îÄ‚îÄ System prompt builder ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _build_system_prompt(self) -> str:
        """
        Fetch the DB schema and embed it into the system prompt.
        Falls back to a generic notice if ClickHouse is unreachable.
        """
        try:
            from tools import _get_ch_client
            tables = _get_ch_client().list_tables()
            schema_block = _build_schema_block(tables)
            schema_section = (
                "–°—Ö–µ–º–∞ —Ç–∞–±–ª–∏—Ü (—Å—Ç–∞—Ç–∏—á–Ω–∞—è, –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –∞–≥–µ–Ω—Ç–∞):\n\n"
                + schema_block
            )
            print(f"‚úÖ Schema loaded: {len(tables)} table(s) embedded in system prompt")
        except Exception as exc:
            schema_section = (
                "–°—Ö–µ–º–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ. "
                "–ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç `list_tables` —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Ç–∞–±–ª–∏—Ü."
            )
            print(f"‚ö†Ô∏è  Could not fetch schema at startup: {exc}")

        return _SYSTEM_PROMPT_TEMPLATE.format(schema_section=schema_section)

    # ‚îÄ‚îÄ‚îÄ Public API ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def analyze(self, user_query: str, session_id: str) -> dict:
        """
        Process a user analytics query for a given session.

        Args:
            user_query: The user's question or request.
            session_id: Unique session identifier (= LangGraph thread_id).
                        Reuse the same session_id across requests to maintain context.

        Returns:
            {
              "success":    bool,
              "session_id": str,
              "text_output": str,         # Final Markdown text from the agent
              "plots":      list[str],    # base64 PNG data URIs from python_analysis
              "tool_calls": list[dict],   # Log of tool invocations
              "error":      str | None,
            }
        """
        config = {"configurable": {"thread_id": session_id}}

        try:
            result = self.graph.invoke(
                {"messages": [HumanMessage(content=user_query)]},
                config=config,
            )

            messages: list = result.get("messages", [])

            text_output = self._extract_final_text(messages)
            plots = self._extract_plots(messages)
            tool_calls = self._extract_tool_calls(messages)

            return {
                "success": True,
                "session_id": session_id,
                "text_output": text_output,
                "plots": plots,
                "tool_calls": tool_calls,
                "error": None,
            }

        except Exception as exc:
            import traceback as tb
            return {
                "success": False,
                "session_id": session_id,
                "text_output": "",
                "plots": [],
                "tool_calls": [],
                "error": str(exc),
                "traceback": tb.format_exc(),
            }

    def get_session_info(self, session_id: str) -> dict:
        """Return basic metadata about a session."""
        try:
            config = {"configurable": {"thread_id": session_id}}
            state = self.graph.get_state(config)
            msgs = state.values.get("messages", []) if state and state.values else []
            user_msgs = sum(1 for m in msgs if isinstance(m, HumanMessage))
            return {
                "session_id": session_id,
                "total_messages": len(msgs),
                "user_turns": user_msgs,
                "has_history": user_msgs > 0,
            }
        except Exception:
            return {
                "session_id": session_id,
                "total_messages": 0,
                "user_turns": 0,
                "has_history": False,
            }

    def cleanup_temp_files(self) -> int:
        """Delete Parquet files older than TEMP_FILE_TTL_SECONDS. Returns count deleted."""
        cutoff = time.time() - TEMP_FILE_TTL_SECONDS
        deleted = 0
        for f in TEMP_DIR.glob("*.parquet"):
            try:
                if f.stat().st_mtime < cutoff:
                    f.unlink()
                    deleted += 1
            except OSError:
                pass
        if deleted:
            print(f"üóëÔ∏è  Deleted {deleted} expired parquet file(s)")
        return deleted

    # ‚îÄ‚îÄ‚îÄ Private helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _extract_final_text(self, messages: list) -> str:
        """Return content of the last AIMessage that has non-empty text."""
        for msg in reversed(messages):
            if not isinstance(msg, AIMessage):
                continue
            content = msg.content
            if isinstance(content, str) and content.strip():
                return content
            if isinstance(content, list):
                parts = [
                    block["text"]
                    for block in content
                    if isinstance(block, dict) and block.get("type") == "text"
                ]
                text = "\n".join(parts).strip()
                if text:
                    return text
        return ""

    def _extract_plots(self, messages: list) -> list[str]:
        """
        Extract base64 PNG plots from python_analysis ToolMessages
        that belong to the CURRENT agent run (after the last HumanMessage).
        """
        last_human_idx = -1
        for i, msg in enumerate(messages):
            if isinstance(msg, HumanMessage):
                last_human_idx = i

        if last_human_idx < 0:
            return []

        plots: list[str] = []
        for msg in messages[last_human_idx:]:
            if not isinstance(msg, ToolMessage):
                continue
            if (getattr(msg, "name", "") or "") != "python_analysis":
                continue
            artifact = getattr(msg, "artifact", None)
            if isinstance(artifact, list):
                plots.extend(artifact)

        return plots

    def _extract_tool_calls(self, messages: list) -> list[dict]:
        """Extract a compact log of tool calls made during the current run."""
        last_human_idx = -1
        for i, msg in enumerate(messages):
            if isinstance(msg, HumanMessage):
                last_human_idx = i

        if last_human_idx < 0:
            return []

        tool_calls: list[dict] = []
        for msg in messages[last_human_idx:]:
            if not isinstance(msg, AIMessage):
                continue
            for tc in getattr(msg, "tool_calls", []):
                name = tc.get("name", "")
                args = tc.get("args", {})
                compact_args = {
                    k: (v[:300] + "‚Ä¶" if isinstance(v, str) and len(v) > 300 else v)
                    for k, v in args.items()
                }
                tool_calls.append({"tool": name, "input": compact_args})

        return tool_calls


# ‚îÄ‚îÄ‚îÄ Global singleton ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
_agent: Optional[AnalyticsAgent] = None


def get_agent() -> AnalyticsAgent:
    """Return (or create) the global AnalyticsAgent instance."""
    global _agent
    if _agent is None:
        _agent = AnalyticsAgent()
    return _agent
