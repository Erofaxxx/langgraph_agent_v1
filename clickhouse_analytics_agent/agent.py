"""
LangGraph-based ClickHouse Analytics Agent.

Architecture:
  - LLM  : Claude Sonnet 4.6 via OpenRouter (ChatOpenAI adapter)
  - Graph : LangGraph create_react_agent (tool-calling loop)
  - Memory: SqliteSaver checkpointer â€” persists full conversation per session_id
  - Tools : list_tables, clickhouse_query, python_analysis

Session isolation:
  Every API request carries a session_id (= LangGraph thread_id).
  SqliteSaver stores the message state keyed by thread_id.
  Multiple concurrent sessions do NOT interfere with each other.
"""

import json
import time
import sqlite3
from copy import copy
from typing import Optional

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.prebuilt import create_react_agent

from config import (
    DB_PATH,
    MAX_AGENT_ITERATIONS,
    MAX_TOKENS,
    MODEL,
    OPENROUTER_API_KEY,
    TEMP_DIR,
    TEMP_FILE_TTL_SECONDS,
)
from tools import TOOLS

# â”€â”€â”€ Context compression helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _compress_tool_message(msg: ToolMessage) -> ToolMessage:
    """
    Replace a ToolMessage's content with a compact version.

    Called only for ToolMessages from PREVIOUS turns so the LLM receives
    minimal but sufficient information about past tool results.

    Compression strategy per tool:
      list_tables    â†’ keep table+column names, drop types   (~60â€“70% smaller)
      clickhouse_query â†’ keep metadata only, drop preview rows (~40â€“50% smaller)
      python_analysis  â†’ keep result summary only, drop stdout  (~50â€“80% smaller)
    """
    tool_name = getattr(msg, "name", "") or ""
    content = msg.content

    try:
        if tool_name == "list_tables":
            data = json.loads(content)
            # Drop column types â€” the LLM already used them to write SQL in that turn
            compact = [
                {"table": t["table"], "columns": [c["name"] for c in t.get("columns", [])]}
                for t in data
            ]
            new_content = json.dumps(compact, ensure_ascii=False)

        elif tool_name == "clickhouse_query":
            data = json.loads(content)
            # Drop preview_first_5_rows and dtypes â€” already analysed in this turn
            new_content = json.dumps(
                {
                    "success": data.get("success"),
                    "row_count": data.get("row_count"),
                    "columns": data.get("columns"),
                    "parquet_path": data.get("parquet_path"),
                },
                ensure_ascii=False,
            )

        elif tool_name == "python_analysis":
            data = json.loads(content)
            # Drop stdout logs â€” keep only the final result (capped at 500 chars)
            result_text = data.get("result") or ""
            new_content = json.dumps(
                {
                    "success": data.get("success"),
                    "result": result_text[:500] + ("â€¦" if len(result_text) > 500 else ""),
                },
                ensure_ascii=False,
            )

        else:
            return msg

    except Exception:
        # If parsing fails, return the original message unchanged
        return msg

    try:
        return msg.model_copy(update={"content": new_content})
    except Exception:
        new_msg = copy(msg)
        new_msg.content = new_content
        return new_msg


def _build_messages_for_llm(state: dict) -> list:
    """
    Prepare the message list for each LLM call:
      1. Prepend system prompt (not stored in the checkpoint).
      2. Compress ToolMessages from previous turns to reduce token usage.

    The SqliteSaver checkpoint always stores the full content â€” compression
    only affects what is actually sent to the model.
    """
    messages = state.get("messages", [])

    # Find the start of the current turn (index of the last HumanMessage)
    current_turn_start = 0
    for i, msg in enumerate(messages):
        if isinstance(msg, HumanMessage):
            current_turn_start = i

    compressed: list = []
    for i, msg in enumerate(messages):
        if i < current_turn_start and isinstance(msg, ToolMessage):
            compressed.append(_compress_tool_message(msg))
        else:
            compressed.append(msg)

    return [SystemMessage(content=SYSTEM_PROMPT)] + compressed


# â”€â”€â”€ System Prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SYSTEM_PROMPT = """Ð¢Ñ‹ â€” Ð»ÑƒÑ‡ÑˆÐ¸Ð¹ Ð² Ð¼Ð¸Ñ€Ðµ Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸Ðº Ñ€ÐµÐºÐ»Ð°Ð¼Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…. Ð Ð°Ð±Ð¾Ñ‚Ð°ÐµÑˆÑŒ Ñ ClickHouse-Ð±Ð°Ð·Ð¾Ð¹ ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ð¸.
Ð¢Ð²Ð¾Ñ Ð·Ð°Ð´Ð°Ñ‡Ð° â€” Ð¾Ñ‚Ð²ÐµÑ‡Ð°Ñ‚ÑŒ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð¼Ð°Ñ€ÐºÐµÑ‚Ð¾Ð»Ð¾Ð³Ð° Ð¿Ð¾ Ð´Ð°Ð½Ð½Ñ‹Ð¼: Ñ‚Ñ€Ð°Ñ„Ð¸Ðº, Ð¿Ð¾ÐºÑƒÐ¿ÐºÐ¸, ÐºÐ°Ð¼Ð¿Ð°Ð½Ð¸Ð¸, Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð².

Ð¡Ñ‚Ð¸Ð»ÑŒ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹: Ñ‚Ñ‹ Ð½Ð°Ñ…Ð¾Ð´Ð¸ÑˆÑŒÑÑ Ð²Ð½ÑƒÑ‚Ñ€Ð¸ Ñ€Ð°Ð±Ð¾Ñ‡ÐµÐ³Ð¾ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ° â€” Ð¼Ð°Ñ€ÐºÐµÑ‚Ð¾Ð»Ð¾Ð³ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ñ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸ ÐºÐ°Ð¶Ð´Ñ‹Ð¹ Ð´ÐµÐ½ÑŒ, Ð·Ð°Ð´Ð°Ñ‘Ñ‚ Ð¼Ð½Ð¾Ð³Ð¾ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð¿Ð¾Ð´Ñ€ÑÐ´, Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ÑÑ Ðº Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ð¼ Ñ‚ÐµÐ¼Ð°Ð¼, ÑƒÑ‚Ð¾Ñ‡Ð½ÑÐµÑ‚. Ð¢Ñ‹ Ñ‡Ð°ÑÑ‚ÑŒ ÑÑ‚Ð¾Ð³Ð¾ Ð¿Ð¾Ñ‚Ð¾ÐºÐ°, Ð½Ðµ Ñ€Ð°Ð·Ð¾Ð²Ñ‹Ð¹ Ð¾Ñ‚Ñ‡Ñ‘Ñ‚. ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¾ Ð¸ Ð¿Ð¾ Ð´ÐµÐ»Ñƒ â€” ÐºÐ°Ðº ÐºÐ¾Ð»Ð»ÐµÐ³Ð°, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ ÑƒÐ¶Ðµ Ð² ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ðµ.

### ÐŸÑ€Ð¸Ð½Ñ†Ð¸Ð¿ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹
Ð¢Ñ‹ Ð²ÐµÐ´Ñ‘ÑˆÑŒ Ñ€Ð°ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ, Ð° Ð½Ðµ Ð¾Ñ‚Ð²ÐµÑ‡Ð°ÐµÑˆÑŒ Ð½Ð° Ð¸Ð·Ð¾Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹. Ð”ÐµÑ€Ð¶Ð¸ Ð½Ð¸Ñ‚ÑŒ:
* ÐŸÐ¾Ð¼Ð½Ð¸ Ñ‡Ñ‚Ð¾ ÑƒÐ¶Ðµ Ð²Ñ‹ÑÑÐ½Ð¸Ð»Ð¸ Ð² ÑÑ‚Ð¾Ð¹ ÑÐµÑÑÐ¸Ð¸ â€” Ð½Ðµ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€ÑÐ¹, Ð¾Ð¿Ð¸Ñ€Ð°Ð¹ÑÑ
* Ð•ÑÐ»Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ñ€Ð¾Ñ‚Ð¸Ð²Ð¾Ñ€ÐµÑ‡Ð°Ñ‚ Ð·Ð´Ñ€Ð°Ð²Ð¾Ð¼Ñƒ ÑÐ¼Ñ‹ÑÐ»Ñƒ â€” ÑÐºÐ°Ð¶Ð¸ Ð¿ÐµÑ€Ð²Ñ‹Ð¼, Ð½Ðµ Ð¶Ð´Ð¸ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ°
* ÐŸÐ¾ÑÐ»Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð²Ð¸Ð´ÑŒ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ Ð»Ð¾Ð³Ð¸Ñ‡Ð½Ñ‹Ð¹ ÑˆÐ°Ð³ â€” Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶Ð¸ ÐµÐ³Ð¾ Ð¾Ð´Ð½Ð¾Ð¹ ÑÑ‚Ñ€Ð¾ÐºÐ¾Ð¹
* ÐÐµ Ð¿Ñ€Ð¸Ð½Ð¸Ð¼Ð°Ð¹ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð·Ð° Ð¸ÑÑ‚Ð¸Ð½Ñƒ Ð±ÐµÐ· Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸: Ð°Ð½Ð¾Ð¼Ð°Ð»Ð¸Ñ, Ð¼Ð°Ð»Ð°Ñ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ°, Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð»Ð¾Ð³Ð¸Ñ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ â€” Ð²ÑÑ‘ Ð¿Ð¾Ð´ ÑÐ¾Ð¼Ð½ÐµÐ½Ð¸ÐµÐ¼ Ð¿Ð¾ÐºÐ° Ð½Ðµ Ð¾Ð±ÑŠÑÑÐ½ÐµÐ½Ð¾

## Ð Ð°Ð±Ð¾Ñ‡Ð¸Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ (Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÑÐ¹ ÑÑ‚Ñ€Ð¾Ð³Ð¾ Ð¿Ð¾ Ð¿Ð¾Ñ€ÑÐ´ÐºÑƒ):

### 1. ÐŸÐ¾Ð½ÑÑ‚ÑŒ Ð·Ð°Ð¿Ñ€Ð¾Ñ
ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»Ð¸ Ñ‚Ð¸Ð¿ â€” Ð¾Ñ‚ ÑÑ‚Ð¾Ð³Ð¾ Ð·Ð°Ð²Ð¸ÑÐ¸Ñ‚ Ð²ÑÑ‘ Ð¾ÑÑ‚Ð°Ð»ÑŒÐ½Ð¾Ðµ:

- Ð¤Ð°ÐºÑ‚ ("ÑÐºÐ¾Ð»ÑŒÐºÐ¾", "Ð¿Ð¾ÐºÐ°Ð¶Ð¸", "Ñ‚Ð¾Ð¿") â†’ Ð¾Ð´Ð½Ð° Ñ†Ð¸Ñ„Ñ€Ð° Ð¸Ð»Ð¸ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ð°, Ð±ÐµÐ· Ð²Ñ‹Ð²Ð¾Ð´Ð¾Ð²
- ÐÐ½Ð°Ð»Ð¸Ð· ("Ð¿Ð¾Ñ‡ÐµÐ¼Ñƒ", "ÑÑ€Ð°Ð²Ð½Ð¸", "ÐµÑÑ‚ÑŒ Ð»Ð¸ Ñ€Ð°Ð·Ð½Ð¸Ñ†Ð°") â†’ Ð´Ð°Ð½Ð½Ñ‹Ðµ + 1â€“2 Ð¸Ð½ÑÐ°Ð¹Ñ‚Ð°
- Ð˜Ð½Ñ‚ÐµÑ€Ð¿Ñ€ÐµÑ‚Ð°Ñ†Ð¸Ñ ("ÑÑ‚Ð¾ Ð½Ð¾Ñ€Ð¼Ð°?", "Ñ…Ð¾Ñ€Ð¾ÑˆÐ¾ Ð¸Ð»Ð¸ Ð¿Ð»Ð¾Ñ…Ð¾?") â†’ Ð¾Ð´Ð½Ð° Ð²Ð¸Ñ‚Ñ€Ð¸Ð½Ð° + Ð¼Ð°Ñ€ÐºÐµÑ‚Ð¸Ð½Ð³Ð¾Ð²Ð°Ñ Ð»Ð¾Ð³Ð¸ÐºÐ°, Ð±ÐµÐ· Ñ‚ÑÐ¶Ñ‘Ð»Ñ‹Ñ… JOIN-Ð¾Ð²
- Drill-down ("Ñ€Ð°Ð·Ð±ÐµÑ€Ð¸", "Ð´ÐµÑ‚Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹") â†’ Ð¿Ð¾Ð»Ð½Ð°Ñ Ð´ÐµÑ‚Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÑƒÐ¼ÐµÑÑ‚Ð½Ð°
- Ð£Ñ‚Ð¾Ñ‡Ð½ÐµÐ½Ð¸Ðµ Ðº Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰ÐµÐ¼Ñƒ â†’ ÑÐ½Ð°Ñ‡Ð°Ð»Ð° Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑŒ, Ð¼Ð¾Ð¶Ð½Ð¾ Ð»Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð¸Ñ‚ÑŒ Ð¸Ð· ÑƒÐ¶Ðµ Ð²Ñ‹Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…; Ð² Ð±Ð°Ð·Ñƒ â€” Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸ Ð½ÐµÑ‚

### 2. Ð˜Ð·ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÑ…ÐµÐ¼Ñƒ (Ð¢ÐžÐ›Ð¬ÐšÐž Ð¿Ñ€Ð¸ Ð¿ÐµÑ€Ð²Ð¾Ð¼ Ð·Ð°Ð¿Ñ€Ð¾ÑÐµ Ð² ÑÐµÑÑÐ¸Ð¸)

Ð•ÑÐ»Ð¸ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð° Ñ‚Ð°Ð±Ð»Ð¸Ñ† ÐµÑ‰Ñ‘ Ð½ÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð° â€” Ð²Ñ‹Ð·Ð¾Ð²Ð¸ `list_tables`.

Ð•ÑÐ»Ð¸ Ð¾Ð½Ð° ÑƒÐ¶Ðµ Ð¸Ð·Ð²ÐµÑÑ‚Ð½Ð° Ð¸Ð· Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸ Ð´Ð¸Ð°Ð»Ð¾Ð³Ð° â€” ÐŸÐ ÐžÐŸÐ£Ð¡Ð¢Ð˜ ÑÑ‚Ð¾Ñ‚ ÑˆÐ°Ð³.

### 3. Ð’Ñ‹Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð· ClickHouse
Ð’Ñ‹Ð·Ð¾Ð²Ð¸ `clickhouse_query` Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¼ SQL:
* ÐÐ³Ñ€ÐµÐ³Ð¸Ñ€ÑƒÐ¹ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ñ€ÑÐ¼Ð¾ Ð² SQL (SUM, COUNT, AVG, GROUP BY) â€” ClickHouse Ð¾Ñ‡ÐµÐ½ÑŒ Ð±Ñ‹ÑÑ‚Ñ€
* Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐ¹ Ð² WHERE â€” Ð½Ðµ Ð²Ñ‹Ð³Ñ€ÑƒÐ¶Ð°Ð¹ Ð»Ð¸ÑˆÐ½ÐµÐµ
* LIMIT: Ð¾Ð±Ñ‹Ñ‡Ð½Ð¾ 1000â€“10000; Ð´Ð¾ 50000 Ð´Ð»Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ð²Ñ‹Ð±Ð¾Ñ€Ð¾Ðº
* Ð¤ÑƒÐ½ÐºÑ†Ð¸Ð¸: toStartOfMonth(), toYear(), toDayOfWeek(), arrayJoin() Ð¸ Ñ‚.Ð´.
* Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸ `parquet_path` Ð¸Ð· Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð´Ð»Ñ python_analysis
* ÐÐ°Ñ‡Ð¸Ð½Ð°Ð¹ Ñ Ð¾Ð´Ð½Ð¾Ð¹ Ð²Ð¸Ñ‚Ñ€Ð¸Ð½Ñ‹. JOIN â€” Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸ Ð±ÐµÐ· Ð½ÐµÐ³Ð¾ Ð¿Ñ€Ð¸Ð½Ñ†Ð¸Ð¿Ð¸Ð°Ð»ÑŒÐ½Ð¾ Ð½Ðµ Ñ€ÐµÑˆÐ¸Ñ‚ÑŒ
* ÐŸÑ€Ð¸ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ â€” ÑƒÐºÐ°Ð·Ñ‹Ð²Ð°Ð¹ Ð² Ð¾Ñ‚Ð²ÐµÑ‚Ðµ Ð¿Ð¾ ÐºÐ°ÐºÐ¾Ð¼Ñƒ Ð¿Ð¾Ð»ÑŽ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÑˆÑŒ

### 4. ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð² Python
Ð’Ñ‹Ð·Ð¾Ð²Ð¸ `python_analysis` Ð´Ð»Ñ Ñ€Ð°ÑÑ‡Ñ‘Ñ‚Ð¾Ð²:
* Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐ¹ Markdown-Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹
* Ð¡Ñ‡Ð¸Ñ‚Ð°Ð¹ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ (CTR, CPC, CPM, ROAS, CR, CPA)
* Ð£ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°Ð¹ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½ÑƒÑŽ `result` Ñ Ð¸Ñ‚Ð¾Ð³Ð¾Ð²Ñ‹Ð¼ Markdown-Ð²Ñ‹Ð²Ð¾Ð´Ð¾Ð¼
* Ð“Ñ€Ð°Ñ„Ð¸Ðº â€” Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¿Ñ€Ð¾ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÑƒ, Ñ‚Ñ€ÐµÐ½Ð´ Ð¸Ð»Ð¸ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸Ðµ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… ÑÑƒÑ‰Ð½Ð¾ÑÑ‚ÐµÐ¹. ÐÐ° Ñ„Ð°ÐºÑ‚Ñ‹ Ð¸ Ñ€Ð°Ð·Ð¾Ð²Ñ‹Ðµ Ñ†Ð¸Ñ„Ñ€Ñ‹ â€” Ð½Ðµ Ð½ÑƒÐ¶ÐµÐ½

ÐŸÑ€Ð¸ Ð°Ð½Ð°Ð»Ð¸Ð·Ðµ:
* Ð•ÑÐ»Ð¸ n < 5 â€” Ð¿Ð¾Ð¼ÐµÑ‡Ð°Ð¹ âš ï¸, Ð²Ñ‹Ð²Ð¾Ð´Ð¾Ð² Ð½Ðµ ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÑŒ
* ÐŸÑ€Ð¸ Ñ€Ð°Ð½Ð¶Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ð¸ Ð¿Ð¾ ÑÑ€ÐµÐ´Ð½ÐµÐ¼Ñƒ Ñ‡ÐµÐºÑƒ Ð¸Ð»Ð¸ CR â€” Ð²ÑÐµÐ³Ð´Ð° Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°Ð¹ n (ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð·Ð°ÐºÐ°Ð·Ð¾Ð²/ÑÐµÑÑÐ¸Ð¹), Ð¸Ð½Ð°Ñ‡Ðµ Ñ‚Ð¾Ð¿ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ð±ÐµÑÑÐ¼Ñ‹ÑÐ»ÐµÐ½ÐµÐ½
* ÐÐ½Ð¾Ð¼Ð°Ð»Ð¸Ð¸ â€” Ð¸ÑÑÐ»ÐµÐ´ÑƒÐ¹, Ð½Ðµ Ð¸Ð³Ð½Ð¾Ñ€Ð¸Ñ€ÑƒÐ¹. ÐžÐ´Ð½Ð° ÑÑ‚Ñ€Ð¾ÐºÐ° Ñ 90% Ð²Ñ‹Ñ€ÑƒÑ‡ÐºÐ¸ â€” ÑÑ‚Ð¾ ÑÐ¸Ð³Ð½Ð°Ð», Ð½Ðµ Ð½Ð¾Ñ€Ð¼Ð°
* Ð•ÑÐ»Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð²Ñ‹Ð²Ð¾Ð´Ð° Ð½ÐµÐ´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ â€” ÑÐºÐ°Ð¶Ð¸ ÑÑ‚Ð¾ Ð¿Ñ€ÑÐ¼Ð¾ Ð¸ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶Ð¸ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ ÑˆÐ°Ð³

### 5. Ð¡Ñ„Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚
Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚ Ð·Ð°Ð²Ð¸ÑÐ¸Ñ‚ Ð¾Ñ‚ Ñ‚Ð¸Ð¿Ð° Ð·Ð°Ð¿Ñ€Ð¾ÑÐ° (ÑÐ¼. ÑˆÐ°Ð³ 1):
* Ð¤Ð°ÐºÑ‚ â†’ Ð¿Ñ€ÑÐ¼Ð¾Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¿ÐµÑ€Ð²Ñ‹Ð¼ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸ÐµÐ¼. Ð¢Ð°Ð±Ð»Ð¸Ñ†Ð° ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð°. Ð’ÑÑ‘.
* ÐÐ½Ð°Ð»Ð¸Ð· â†’ Ð´Ð°Ð½Ð½Ñ‹Ðµ + Ð¼Ð°ÐºÑÐ¸Ð¼ÑƒÐ¼ 2 Ð¸Ð½ÑÐ°Ð¹Ñ‚Ð°. Ð‘ÐµÐ· Ñ€Ð°Ð·Ð´ÐµÐ»Ð° "ÐšÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ Ð²Ñ‹Ð²Ð¾Ð´Ñ‹" â€” ÐµÑÐ»Ð¸ Ð¸Ð½ÑÐ°Ð¹Ñ‚ ÑƒÐ¶Ðµ Ð² Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ðµ, Ð½Ðµ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€ÑÐ¹
* Ð˜Ð½Ñ‚ÐµÑ€Ð¿Ñ€ÐµÑ‚Ð°Ñ†Ð¸Ñ â†’ Ð²Ñ‹Ð²Ð¾Ð´ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… + Ð¼Ð°Ñ€ÐºÐµÑ‚Ð¸Ð½Ð³Ð¾Ð²Ð°Ñ Ð»Ð¾Ð³Ð¸ÐºÐ°. Ð‘ÐµÐ· Ð´Ð¾Ð¼Ñ‹ÑÐ»Ð¾Ð² Ð¾ Ð±Ð¸Ð·Ð½ÐµÑÐµ
* Drill-down â†’ Ð¿Ð¾Ð»Ð½Ð°Ñ Ð´ÐµÑ‚Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ, Ð³Ð¸Ð¿Ð¾Ñ‚ÐµÐ·Ñ‹, Ð¾Ð±ÑŠÑÑÐ½ÐµÐ½Ð¸Ðµ Ð°Ð½Ð¾Ð¼Ð°Ð»Ð¸Ð¹

Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¸ â€” Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿Ñ€Ð¸ Ñ‚Ñ€Ñ‘Ñ… ÑƒÑÐ»Ð¾Ð²Ð¸ÑÑ… Ð¾Ð´Ð½Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾:
1. Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð»Ñ Ð½ÐµÑ‘ ÐµÑÑ‚ÑŒ Ð² Ð²Ñ‹Ð³Ñ€ÑƒÐ·ÐºÐµ
2. ÐšÐ°Ð½Ð°Ð»/Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚ Ð²Ð¸Ð´ÐµÐ½ Ð² Ð´Ð°Ð½Ð½Ñ‹Ñ… (Ð½Ðµ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°Ñ‚ÑŒ Ñ‚Ð¾, Ñ‡ÐµÐ³Ð¾ Ð½ÐµÑ‚)
3. Ð”Ð»Ñ Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ â€” ÐµÑÑ‚ÑŒ CR Ð¸Ð»Ð¸ spend Ð¿Ð¾ ÑÑ‚Ð¾Ð¹ ÑÑƒÑ‰Ð½Ð¾ÑÑ‚Ð¸

Ð•ÑÐ»Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð½ÐµÐ´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ â†’ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶Ð¸ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ ÑˆÐ°Ð³: "ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ CR ÑÑ‚Ð¾Ð¹ ÐºÐ°Ð¼Ð¿Ð°Ð½Ð¸Ð¸?"

ÐŸÑ€Ð¾Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ: ÐµÑÐ»Ð¸ Ð² Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð²Ð¸Ð´ÐµÐ½ Ð½ÐµÑ‚Ñ€Ð¸Ð²Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½ â€” Ð½Ð°Ð·Ð¾Ð²Ð¸ ÐµÐ³Ð¾, Ð´Ð°Ð¶Ðµ ÐµÑÐ»Ð¸ Ð½Ðµ ÑÐ¿Ñ€Ð¾ÑÐ¸Ð»Ð¸. ÐžÐ´Ð¸Ð½ Ð¸Ð½ÑÐ°Ð¹Ñ‚ ÑÐ²ÐµÑ€Ñ… Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ° â€” Ð½Ð¾Ñ€Ð¼Ð°. Ð”Ð²Ð° Ð¸ Ð±Ð¾Ð»ÑŒÑˆÐµ â€” ÑƒÐ¶Ðµ Ð±Ð°Ð»Ð»Ð°ÑÑ‚.

ÐšÐ°Ð¶Ð´Ð¾Ðµ ÑÐ»Ð¾Ð²Ð¾ Ð² Ð²Ñ‹Ð²Ð¾Ð´Ðµ Ð´Ð¾Ð»Ð¶Ð½Ð¾ Ð½ÐµÑÑ‚Ð¸ ÑÐ¼Ñ‹ÑÐ». ÐÐ¸ÐºÐ°ÐºÐ¸Ñ… Ð¸Ñ‚Ð¾Ð³Ð¾Ð²Ñ‹Ñ… Ð±Ð»Ð¾ÐºÐ¾Ð² Ñ ÑÐ¼Ð¾Ð´Ð·Ð¸, Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð¾Ð², Ð¾Ð±Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ñ€Ð°Ð´Ð¸ Ð¾Ð±Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹.

## ÐŸÑ€Ð°Ð²Ð¸Ð»Ð° Python-ÐºÐ¾Ð´Ð°:
1. `df` ÑƒÐ¶Ðµ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½ â€” ÐÐ• Ð²Ñ‹Ð·Ñ‹Ð²Ð°Ð¹ pd.read_parquet()
2. Ð’Ð¡Ð•Ð“Ð”Ð ÑƒÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°Ð¹ `result` (Markdown ÑÑ‚Ñ€Ð¾ÐºÐ° Ñ Ð¸Ñ‚Ð¾Ð³Ð¾Ð¼)
3. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ print() Ð´Ð»Ñ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ ÑˆÐ°Ð³Ð¾Ð²: print("ðŸ“Š Ð¨Ð°Ð³ 1: ...")
4. ÐŸÐ¾Ð´Ð¿Ð¸ÑÑ‹Ð²Ð°Ð¹ Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ¸ Ð½Ð° Ð Ð£Ð¡Ð¡ÐšÐžÐœ: plt.title(), plt.xlabel(), plt.ylabel()
5. Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€ÑƒÐ¹ Ñ‡Ð¸ÑÐ»Ð°: f"{value:,.0f}" (Ñ†ÐµÐ»Ñ‹Ðµ), f"{value:,.2f}" (Ð´Ñ€Ð¾Ð±Ð½Ñ‹Ðµ)
6. ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°Ð¹ Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ¸: df.dropna() Ð¸Ð»Ð¸ df.fillna(0)
7. Ð”Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ° â€” plt.tight_layout() Ð¿ÐµÑ€ÐµÐ´ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¼
8. Ð“Ñ€Ð°Ñ„Ð¸Ðº ÑÑ‚Ñ€Ð¾Ð¹ Ð¢ÐžÐ›Ð¬ÐšÐž ÐµÑÐ»Ð¸ Ð¾Ð½ ÑÐ²Ð½Ð¾ Ð½ÑƒÐ¶ÐµÐ½ Ð¿Ð¾ Ñ‚Ð¸Ð¿Ñƒ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ° (ÑÐ¼. ÑˆÐ°Ð³ 4) â€” Ð½Ðµ Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ

## Ð ÐµÐºÐ»Ð°Ð¼Ð½Ñ‹Ðµ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸
Ð¤Ð¾Ñ€Ð¼ÑƒÐ»Ñ‹ Ð½Ðµ Ð½ÑƒÐ¶Ð½Ð¾ Ð²Ð¾ÑÐ¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÑŒ Ð² ÐºÐ°Ð¶Ð´Ð¾Ð¼ Ð¾Ñ‚Ð²ÐµÑ‚Ðµ. ÐÐ¾:
* Ð•ÑÐ»Ð¸ ÑÑ‡Ð¸Ñ‚Ð°ÐµÑˆÑŒ Ð½ÐµÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½ÑƒÑŽ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÑƒ Ð¸Ð»Ð¸ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð½ÑƒÑŽ â€” Ð¿Ð¾ÐºÐ°Ð¶Ð¸ Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ñƒ Ð¾Ð´Ð¸Ð½ Ñ€Ð°Ð·
* Ð•ÑÐ»Ð¸ Ð²Ð²Ð¾Ð´Ð¸ÑˆÑŒ Ð°Ð±Ð±Ñ€ÐµÐ²Ð¸Ð°Ñ‚ÑƒÑ€Ñƒ ÐºÐ¾Ñ‚Ð¾Ñ€ÑƒÑŽ Ð¼Ð°Ñ€ÐºÐµÑ‚Ð¾Ð»Ð¾Ð³ Ð¼Ð¾Ð³ Ð½Ðµ Ð·Ð½Ð°Ñ‚ÑŒ â€” Ñ€Ð°ÑÑˆÐ¸Ñ„Ñ€ÑƒÐ¹ Ð¿Ñ€Ð¸ Ð¿ÐµÑ€Ð²Ð¾Ð¼ ÑƒÐ¿Ð¾Ð¼Ð¸Ð½Ð°Ð½Ð¸Ð¸
* Ð•ÑÐ»Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð½ÐµÑ‚ (Ð½ÐµÑ‚ Ñ€Ð°ÑÑ…Ð¾Ð´Ð° â†’ Ð½ÐµÑ‚ CPC/CPA/ROAS) â€” ÑÐºÐ°Ð¶Ð¸ Ð¿Ñ€ÑÐ¼Ð¾, Ð½Ðµ Ð´Ð¾Ð´ÑƒÐ¼Ñ‹Ð²Ð°Ð¹

## Ð¡Ñ‚Ð¸Ð»ÑŒ Ð¾Ñ‚Ð²ÐµÑ‚Ð°
* Markdown: Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¸ ##/###, Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹, Ð¶Ð¸Ñ€Ð½Ñ‹Ð¹ Ð´Ð»Ñ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ñ… Ñ†Ð¸Ñ„Ñ€
* Ð­Ð¼Ð¾Ð´Ð·Ð¸ â€” Ñ‚Ð¾Ð»ÑŒÐºÐ¾ âš ï¸ Ð´Ð»Ñ Ð¿Ñ€ÐµÐ´ÑƒÐ¿Ñ€ÐµÐ¶Ð´ÐµÐ½Ð¸Ð¹. Ð‘Ð¾Ð»ÑŒÑˆÐµ Ð½Ð¸Ð³Ð´Ðµ
* Ð§Ð¸ÑÐ»Ð° Ñ Ñ€Ð°Ð·Ð´ÐµÐ»Ð¸Ñ‚ÐµÐ»ÑÐ¼Ð¸ Ñ‚Ñ‹ÑÑÑ‡: 1 234 567
* Ð¯Ð·Ñ‹Ðº â€” Ñ€ÑƒÑÑÐºÐ¸Ð¹
* ÐšÐ¾Ð½ÐºÑ€ÐµÑ‚Ð¸ÐºÐ°: Ñ†Ð¸Ñ„Ñ€Ñ‹, Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ°, ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸Ðµ â€” Ð±ÐµÐ· Ð²Ð¾Ð´Ñ‹
"""


class AnalyticsAgent:
    """
    Wraps LangGraph ReAct agent with:
      - Claude Sonnet 4.6 via OpenRouter
      - SqliteSaver for session memory
      - Helper methods to extract plots and tool-call logs from agent output
    """

    def __init__(self) -> None:
        if not OPENROUTER_API_KEY:
            raise ValueError(
                "OPENROUTER_API_KEY is not set in .env. "
                "Get your key at https://openrouter.ai"
            )

        # â”€â”€ LLM via OpenRouter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self.llm = ChatOpenAI(
            model=MODEL,
            api_key=OPENROUTER_API_KEY,
            base_url="https://openrouter.ai/api/v1",
            max_tokens=MAX_TOKENS,
            default_headers={
                "HTTP-Referer": "https://server.asktab.ru",
                "X-Title": "ClickHouse Analytics Agent",
            },
        )

        # â”€â”€ SqliteSaver checkpointer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Keeps conversation state per thread_id (= session_id).
        # Thread-safe for concurrent requests.
        conn = sqlite3.connect(str(DB_PATH), check_same_thread=False)
        self.memory = SqliteSaver(conn)


        # â”€â”€ LangGraph ReAct agent â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # _build_messages_for_llm is called before every LLM invocation:
        #   â€¢ prepends the system prompt (not stored in checkpoint)
        #   â€¢ compresses ToolMessages from previous turns to cut token usage
        self.graph = create_react_agent(
            model=self.llm,
            tools=TOOLS,
            prompt=_build_messages_for_llm,
            checkpointer=self.memory,
        )

        print(f"âœ… AnalyticsAgent ready | model: {MODEL} | db: {DB_PATH}")

    # â”€â”€â”€ Public API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def analyze(self, user_query: str, session_id: str) -> dict:
        """
        Process a user analytics query for a given session.

        Args:
            user_query: The user's question or request.
            session_id: Unique session identifier (= LangGraph thread_id).
                        Reuse the same session_id across requests to maintain context.

        Returns:
            {
              "success":    bool,
              "session_id": str,
              "text_output": str,         # Final Markdown text from the agent
              "plots":      list[str],    # base64 PNG data URIs from python_analysis
              "tool_calls": list[dict],   # Log of tool invocations
              "error":      str | None,
            }
        """
        config = {"configurable": {"thread_id": session_id}}

        try:
            # LangGraph invoke â€” sends only the NEW message;
            # history is loaded automatically from SqliteSaver by thread_id.
            result = self.graph.invoke(
                {"messages": [HumanMessage(content=user_query)]},
                config=config,
            )

            messages: list = result.get("messages", [])

            text_output = self._extract_final_text(messages)
            plots = self._extract_plots(messages)
            tool_calls = self._extract_tool_calls(messages)

            return {
                "success": True,
                "session_id": session_id,
                "text_output": text_output,
                "plots": plots,
                "tool_calls": tool_calls,
                "error": None,
            }

        except Exception as exc:
            import traceback as tb
            return {
                "success": False,
                "session_id": session_id,
                "text_output": "",
                "plots": [],
                "tool_calls": [],
                "error": str(exc),
                "traceback": tb.format_exc(),
            }

    def get_session_info(self, session_id: str) -> dict:
        """Return basic metadata about a session."""
        try:
            config = {"configurable": {"thread_id": session_id}}
            state = self.graph.get_state(config)
            msgs = state.values.get("messages", []) if state and state.values else []
            # Count only user-visible exchanges (HumanMessage + AIMessage pairs)
            user_msgs = sum(1 for m in msgs if isinstance(m, HumanMessage))
            return {
                "session_id": session_id,
                "total_messages": len(msgs),
                "user_turns": user_msgs,
                "has_history": user_msgs > 0,
            }
        except Exception:
            return {
                "session_id": session_id,
                "total_messages": 0,
                "user_turns": 0,
                "has_history": False,
            }

    def cleanup_temp_files(self) -> int:
        """Delete Parquet files older than TEMP_FILE_TTL_SECONDS. Returns count deleted."""
        cutoff = time.time() - TEMP_FILE_TTL_SECONDS
        deleted = 0
        for f in TEMP_DIR.glob("*.parquet"):
            try:
                if f.stat().st_mtime < cutoff:
                    f.unlink()
                    deleted += 1
            except OSError:
                pass
        if deleted:
            print(f"ðŸ—‘ï¸  Deleted {deleted} expired parquet file(s)")
        return deleted

    # â”€â”€â”€ Private helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _extract_final_text(self, messages: list) -> str:
        """Return content of the last AIMessage that has non-empty text."""
        for msg in reversed(messages):
            if not isinstance(msg, AIMessage):
                continue
            content = msg.content
            if isinstance(content, str) and content.strip():
                return content
            # Some models return list of content blocks
            if isinstance(content, list):
                parts = [
                    block["text"]
                    for block in content
                    if isinstance(block, dict) and block.get("type") == "text"
                ]
                text = "\n".join(parts).strip()
                if text:
                    return text
        return ""

    def _extract_plots(self, messages: list) -> list[str]:
        """
        Extract base64 PNG plots from python_analysis ToolMessages
        that belong to the CURRENT agent run (after the last HumanMessage).

        Plots are stored in ToolMessage.artifact (not in .content) so they
        are never sent to the LLM, only kept in the checkpoint state for us.
        """
        # Find index of the most recently added HumanMessage
        last_human_idx = -1
        for i, msg in enumerate(messages):
            if isinstance(msg, HumanMessage):
                last_human_idx = i

        if last_human_idx < 0:
            return []

        plots: list[str] = []
        for msg in messages[last_human_idx:]:
            if not isinstance(msg, ToolMessage):
                continue
            tool_name = getattr(msg, "name", "") or ""
            if tool_name != "python_analysis":
                continue
            # Plots are in .artifact (base64 list), NOT in .content
            artifact = getattr(msg, "artifact", None)
            if isinstance(artifact, list):
                plots.extend(artifact)

        return plots

    def _extract_tool_calls(self, messages: list) -> list[dict]:
        """
        Extract a compact log of tool calls made during the current run.
        """
        last_human_idx = -1
        for i, msg in enumerate(messages):
            if isinstance(msg, HumanMessage):
                last_human_idx = i

        if last_human_idx < 0:
            return []

        tool_calls: list[dict] = []
        for msg in messages[last_human_idx:]:
            if not isinstance(msg, AIMessage):
                continue
            for tc in getattr(msg, "tool_calls", []):
                name = tc.get("name", "")
                args = tc.get("args", {})
                # Truncate large args for the log
                compact_args = {
                    k: (v[:300] + "â€¦" if isinstance(v, str) and len(v) > 300 else v)
                    for k, v in args.items()
                }
                tool_calls.append({"tool": name, "input": compact_args})

        return tool_calls


# â”€â”€â”€ Global singleton â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_agent: Optional[AnalyticsAgent] = None


def get_agent() -> AnalyticsAgent:
    """Return (or create) the global AnalyticsAgent instance."""
    global _agent
    if _agent is None:
        _agent = AnalyticsAgent()
    return _agent
