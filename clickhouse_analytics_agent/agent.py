"""
LangGraph-based ClickHouse Analytics Agent.

Architecture:
  - LLM  : Claude Sonnet 4.6 via OpenRouter (ChatOpenAI adapter)
  - Graph : LangGraph create_react_agent (tool-calling loop)
  - Memory: SqliteSaver checkpointer ‚Äî persists full conversation per session_id
  - Tools : list_tables, clickhouse_query, python_analysis

Session isolation:
  Every API request carries a session_id (= LangGraph thread_id).
  SqliteSaver stores the message state keyed by thread_id.
  Multiple concurrent sessions do NOT interfere with each other.
"""

import json
import time
import sqlite3
from copy import copy
from typing import Optional

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.prebuilt import create_react_agent

from config import (
    DB_PATH,
    MAX_AGENT_ITERATIONS,
    MAX_TOKENS,
    MODEL,
    OPENROUTER_API_KEY,
    TEMP_DIR,
    TEMP_FILE_TTL_SECONDS,
)
from tools import TOOLS

# ‚îÄ‚îÄ‚îÄ Context compression helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _compress_tool_message(msg: ToolMessage) -> ToolMessage:
    """
    Replace a ToolMessage's content with a compact version.

    Called only for ToolMessages from PREVIOUS turns so the LLM receives
    minimal but sufficient information about past tool results.

    Compression strategy per tool:
      list_tables    ‚Üí keep table+column names, drop types   (~60‚Äì70% smaller)
      clickhouse_query ‚Üí keep metadata only, drop preview rows (~40‚Äì50% smaller)
      python_analysis  ‚Üí keep result summary only, drop stdout  (~50‚Äì80% smaller)
    """
    tool_name = getattr(msg, "name", "") or ""
    content = msg.content

    try:
        if tool_name == "list_tables":
            data = json.loads(content)
            # Normalise to plain column-name lists.
            # Handles both old format (columns: [{"name": ..., "type": ...}])
            # and new format (columns: ["col1", "col2", ...]) for checkpoint compat.
            compact = []
            for t in data:
                cols = t.get("columns", [])
                if cols and isinstance(cols[0], dict):
                    col_names = [c["name"] for c in cols]
                else:
                    col_names = cols
                compact.append({"table": t["table"], "columns": col_names})
            new_content = json.dumps(compact, ensure_ascii=False)

        elif tool_name == "clickhouse_query":
            data = json.loads(content)
            # Drop preview_first_5_rows and dtypes ‚Äî already analysed in this turn
            new_content = json.dumps(
                {
                    "success": data.get("success"),
                    "row_count": data.get("row_count"),
                    "columns": data.get("columns"),
                    "parquet_path": data.get("parquet_path"),
                },
                ensure_ascii=False,
            )

        elif tool_name == "python_analysis":
            data = json.loads(content)
            # Drop stdout logs ‚Äî keep only the final result (capped at 500 chars)
            result_text = data.get("result") or ""
            new_content = json.dumps(
                {
                    "success": data.get("success"),
                    "result": result_text[:500] + ("‚Ä¶" if len(result_text) > 500 else ""),
                },
                ensure_ascii=False,
            )

        else:
            return msg

    except Exception:
        # If parsing fails, return the original message unchanged
        return msg

    try:
        return msg.model_copy(update={"content": new_content})
    except Exception:
        new_msg = copy(msg)
        new_msg.content = new_content
        return new_msg


def _build_messages_for_llm(state: dict) -> list:
    """
    Prepare the message list for each LLM call:
      1. Prepend system prompt (not stored in the checkpoint).
      2. Compress ToolMessages from previous turns to reduce token usage.
      3. Compress "used" ToolMessages within the current turn ‚Äî those already
         followed by another AIMessage with tool_calls. The most recent
         ToolMessage is always kept uncompressed so the LLM sees full data.

    The SqliteSaver checkpoint always stores the full content ‚Äî compression
    only affects what is actually sent to the model.
    """
    messages = state.get("messages", [])

    # Find the start of the current turn (index of the last HumanMessage)
    current_turn_start = 0
    for i, msg in enumerate(messages):
        if isinstance(msg, HumanMessage):
            current_turn_start = i

    # Within the current turn, find the last AIMessage that issued tool calls.
    # Any ToolMessage that appears BEFORE this index has already been consumed
    # by the LLM and can be safely compressed to save tokens.
    last_tool_calling_ai_idx = -1
    for i in range(len(messages) - 1, current_turn_start - 1, -1):
        m = messages[i]
        if isinstance(m, AIMessage) and getattr(m, "tool_calls", None):
            last_tool_calling_ai_idx = i
            break

    compressed: list = []
    for i, msg in enumerate(messages):
        if isinstance(msg, ToolMessage):
            if i < current_turn_start:
                # Previous turns ‚Äî always compress
                compressed.append(_compress_tool_message(msg))
            elif last_tool_calling_ai_idx > i:
                # Current turn, already consumed (LLM made another tool call after this) ‚Äî compress
                compressed.append(_compress_tool_message(msg))
            else:
                # Current turn, most recent result ‚Äî keep full
                compressed.append(msg)
        else:
            compressed.append(msg)

    return [SystemMessage(content=SYSTEM_PROMPT)] + compressed


# ‚îÄ‚îÄ‚îÄ System Prompt ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
SYSTEM_PROMPT = """–¢—ã ‚Äî –ª—É—á—à–∏–π –≤ –º–∏—Ä–µ –∞–Ω–∞–ª–∏—Ç–∏–∫ —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –†–∞–±–æ—Ç–∞–µ—à—å —Å ClickHouse-–±–∞–∑–æ–π –∫–æ–º–ø–∞–Ω–∏–∏.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –º–∞—Ä–∫–µ—Ç–æ–ª–æ–≥–∞ –ø–æ –¥–∞–Ω–Ω—ã–º: —Ç—Ä–∞—Ñ–∏–∫, –ø–æ–∫—É–ø–∫–∏, –∫–∞–º–ø–∞–Ω–∏–∏, –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–æ–≤.

–°—Ç–∏–ª—å —Ä–∞–±–æ—Ç—ã: —Ç—ã –Ω–∞—Ö–æ–¥–∏—à—å—Å—è –≤–Ω—É—Ç—Ä–∏ —Ä–∞–±–æ—á–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ ‚Äî –º–∞—Ä–∫–µ—Ç–æ–ª–æ–≥ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –¥–∞–Ω–Ω—ã–º–∏ –∫–∞–∂–¥—ã–π –¥–µ–Ω—å, –∑–∞–¥–∞—ë—Ç –º–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ–¥—Ä—è–¥, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –∫ –ø—Ä–µ–¥—ã–¥—É—â–∏–º —Ç–µ–º–∞–º, —É—Ç–æ—á–Ω—è–µ—Ç. –¢—ã —á–∞—Å—Ç—å —ç—Ç–æ–≥–æ –ø–æ—Ç–æ–∫–∞, –Ω–µ —Ä–∞–∑–æ–≤—ã–π –æ—Ç—á—ë—Ç. –û—Ç–≤–µ—á–∞–π –∫–æ—Ä–æ—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É ‚Äî –∫–∞–∫ –∫–æ–ª–ª–µ–≥–∞, –∫–æ—Ç–æ—Ä—ã–π —É–∂–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.

### –ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã
–¢—ã –≤–µ–¥—ë—à—å —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ, –∞ –Ω–µ –æ—Ç–≤–µ—á–∞–µ—à—å –Ω–∞ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã. –î–µ—Ä–∂–∏ –Ω–∏—Ç—å:
* –ü–æ–º–Ω–∏ —á—Ç–æ —É–∂–µ –≤—ã—è—Å–Ω–∏–ª–∏ –≤ —ç—Ç–æ–π —Å–µ—Å—Å–∏–∏ ‚Äî –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–π, –æ–ø–∏—Ä–∞–π—Å—è
* –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∞—Ç –∑–¥—Ä–∞–≤–æ–º—É —Å–º—ã—Å–ª—É ‚Äî —Å–∫–∞–∂–∏ –ø–µ—Ä–≤—ã–º, –Ω–µ –∂–¥–∏ –≤–æ–ø—Ä–æ—Å–∞
* –ü–æ—Å–ª–µ –æ—Ç–≤–µ—Ç–∞ –≤–∏–¥—å —Å–ª–µ–¥—É—é—â–∏–π –ª–æ–≥–∏—á–Ω—ã–π —à–∞–≥ ‚Äî –ø—Ä–µ–¥–ª–æ–∂–∏ –µ–≥–æ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π
* –ù–µ –ø—Ä–∏–Ω–∏–º–∞–π –¥–∞–Ω–Ω—ã–µ –∑–∞ –∏—Å—Ç–∏–Ω—É –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏: –∞–Ω–æ–º–∞–ª–∏—è, –º–∞–ª–∞—è –≤—ã–±–æ—Ä–∫–∞, –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ ‚Äî –≤—Å—ë –ø–æ–¥ —Å–æ–º–Ω–µ–Ω–∏–µ–º –ø–æ–∫–∞ –Ω–µ –æ–±—ä—è—Å–Ω–µ–Ω–æ

## –†–∞–±–æ—á–∏–π –ø—Ä–æ—Ü–µ—Å—Å (–≤—ã–ø–æ–ª–Ω—è–π —Å—Ç—Ä–æ–≥–æ –ø–æ –ø–æ—Ä—è–¥–∫—É):

### 1. –ü–æ–Ω—è—Ç—å –∑–∞–ø—Ä–æ—Å
–û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø ‚Äî –æ—Ç —ç—Ç–æ–≥–æ –∑–∞–≤–∏—Å–∏—Ç –≤—Å—ë –æ—Å—Ç–∞–ª—å–Ω–æ–µ:

- –§–∞–∫—Ç ("—Å–∫–æ–ª—å–∫–æ", "–ø–æ–∫–∞–∂–∏", "—Ç–æ–ø") ‚Üí –æ–¥–Ω–∞ —Ü–∏—Ñ—Ä–∞ –∏–ª–∏ —Ç–∞–±–ª–∏—Ü–∞, –±–µ–∑ –≤—ã–≤–æ–¥–æ–≤
- –ê–Ω–∞–ª–∏–∑ ("–ø–æ—á–µ–º—É", "—Å—Ä–∞–≤–Ω–∏", "–µ—Å—Ç—å –ª–∏ —Ä–∞–∑–Ω–∏—Ü–∞") ‚Üí –¥–∞–Ω–Ω—ã–µ + 1‚Äì2 –∏–Ω—Å–∞–π—Ç–∞
- –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è ("—ç—Ç–æ –Ω–æ—Ä–º–∞?", "—Ö–æ—Ä–æ—à–æ –∏–ª–∏ –ø–ª–æ—Ö–æ?") ‚Üí –æ–¥–Ω–∞ –≤–∏—Ç—Ä–∏–Ω–∞ + –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–∞—è –ª–æ–≥–∏–∫–∞, –±–µ–∑ —Ç—è–∂—ë–ª—ã—Ö JOIN-–æ–≤
- Drill-down ("—Ä–∞–∑–±–µ—Ä–∏", "–¥–µ—Ç–∞–ª–∏–∑–∏—Ä—É–π") ‚Üí –ø–æ–ª–Ω–∞—è –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è —É–º–µ—Å—Ç–Ω–∞
- –£—Ç–æ—á–Ω–µ–Ω–∏–µ –∫ –ø—Ä–µ–¥—ã–¥—É—â–µ–º—É ‚Üí —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—å, –º–æ–∂–Ω–æ –ª–∏ –æ—Ç–≤–µ—Ç–∏—Ç—å –∏–∑ —É–∂–µ –≤—ã–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö; –≤ –±–∞–∑—É ‚Äî —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ—Ç

### 2. –ò–∑—É—á–∏—Ç—å —Å—Ö–µ–º—É (–¢–û–õ–¨–ö–û –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—Ä–æ—Å–µ –≤ —Å–µ—Å—Å–∏–∏)

–ï—Å–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–∞–±–ª–∏—Ü –µ—â—ë –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞ ‚Äî –≤—ã–∑–æ–≤–∏ `list_tables`.

–ï—Å–ª–∏ –æ–Ω–∞ —É–∂–µ –∏–∑–≤–µ—Å—Ç–Ω–∞ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞ ‚Äî –ü–†–û–ü–£–°–¢–ò —ç—Ç–æ—Ç —à–∞–≥.

### 3. –í—ã–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ ClickHouse
–í—ã–∑–æ–≤–∏ `clickhouse_query` —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º SQL:
* –ê–≥—Ä–µ–≥–∏—Ä—É–π –¥–∞–Ω–Ω—ã–µ –ø—Ä—è–º–æ –≤ SQL (SUM, COUNT, AVG, GROUP BY) ‚Äî ClickHouse –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä
* –§–∏–ª—å—Ç—Ä—É–π –≤ WHERE ‚Äî –Ω–µ –≤—ã–≥—Ä—É–∂–∞–π –ª–∏—à–Ω–µ–µ
* LIMIT: –æ–±—ã—á–Ω–æ 1000‚Äì10000; –¥–æ 50000 –¥–ª—è –±–æ–ª—å—à–∏—Ö –≤—ã–±–æ—Ä–æ–∫
* –§—É–Ω–∫—Ü–∏–∏: toStartOfMonth(), toYear(), toDayOfWeek(), arrayJoin() –∏ —Ç.–¥.
* –°–æ—Ö—Ä–∞–Ω–∏ `parquet_path` –∏–∑ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è python_analysis
* –ù–∞—á–∏–Ω–∞–π —Å –æ–¥–Ω–æ–π –≤–∏—Ç—Ä–∏–Ω—ã. JOIN ‚Äî —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –±–µ–∑ –Ω–µ–≥–æ –ø—Ä–∏–Ω—Ü–∏–ø–∏–∞–ª—å–Ω–æ –Ω–µ —Ä–µ—à–∏—Ç—å
* –ü—Ä–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ ‚Äî —É–∫–∞–∑—ã–≤–∞–π –≤ –æ—Ç–≤–µ—Ç–µ –ø–æ –∫–∞–∫–æ–º—É –ø–æ–ª—é —Ñ–∏–ª—å—Ç—Ä—É–µ—à—å

### 4. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ Python
–í—ã–∑–æ–≤–∏ `python_analysis` –¥–ª—è —Ä–∞—Å—á—ë—Ç–æ–≤:
* –§–æ—Ä–º–∏—Ä—É–π Markdown-—Ç–∞–±–ª–∏—Ü—ã
* –°—á–∏—Ç–∞–π –º–µ—Ç—Ä–∏–∫–∏ (CTR, CPC, CPM, ROAS, CR, CPA)
* –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–π –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é `result` —Å –∏—Ç–æ–≥–æ–≤—ã–º Markdown-–≤—ã–≤–æ–¥–æ–º
* –ì—Ä–∞—Ñ–∏–∫ ‚Äî —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –ø—Ä–æ –¥–∏–Ω–∞–º–∏–∫—É, —Ç—Ä–µ–Ω–¥ –∏–ª–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π. –ù–∞ —Ñ–∞–∫—Ç—ã –∏ —Ä–∞–∑–æ–≤—ã–µ —Ü–∏—Ñ—Ä—ã ‚Äî –Ω–µ –Ω—É–∂–µ–Ω

–ü—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ:
* –ï—Å–ª–∏ n < 5 ‚Äî –ø–æ–º–µ—á–∞–π ‚ö†Ô∏è, –≤—ã–≤–æ–¥–æ–≤ –Ω–µ —Å—Ç—Ä–æ–∏—Ç—å
* –ü—Ä–∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø–æ —Å—Ä–µ–¥–Ω–µ–º—É —á–µ–∫—É –∏–ª–∏ CR ‚Äî –≤—Å–µ–≥–¥–∞ –ø–æ–∫–∞–∑—ã–≤–∞–π n (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–∫–∞–∑–æ–≤/—Å–µ—Å—Å–∏–π), –∏–Ω–∞—á–µ —Ç–æ–ø —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–µ–Ω
* –ê–Ω–æ–º–∞–ª–∏–∏ ‚Äî –∏—Å—Å–ª–µ–¥—É–π, –Ω–µ –∏–≥–Ω–æ—Ä–∏—Ä—É–π. –û–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞ —Å 90% –≤—ã—Ä—É—á–∫–∏ ‚Äî —ç—Ç–æ —Å–∏–≥–Ω–∞–ª, –Ω–µ –Ω–æ—Ä–º–∞
* –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã–≤–æ–¥–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ ‚Äî —Å–∫–∞–∂–∏ —ç—Ç–æ –ø—Ä—è–º–æ –∏ –ø—Ä–µ–¥–ª–æ–∂–∏ —Å–ª–µ–¥—É—é—â–∏–π —à–∞–≥

### 5. –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
–§–æ—Ä–º–∞—Ç –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞ (—Å–º. —à–∞–≥ 1):
* –§–∞–∫—Ç ‚Üí –ø—Ä—è–º–æ–π –æ—Ç–≤–µ—Ç –ø–µ—Ä–≤—ã–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º. –¢–∞–±–ª–∏—Ü–∞ –µ—Å–ª–∏ –Ω—É–∂–Ω–∞. –í—Å—ë.
* –ê–Ω–∞–ª–∏–∑ ‚Üí –¥–∞–Ω–Ω—ã–µ + –º–∞–∫—Å–∏–º—É–º 2 –∏–Ω—Å–∞–π—Ç–∞. –ë–µ–∑ —Ä–∞–∑–¥–µ–ª–∞ "–ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã" ‚Äî –µ—Å–ª–∏ –∏–Ω—Å–∞–π—Ç —É–∂–µ –≤ —Ç–∞–±–ª–∏—Ü–µ, –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–π
* –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è ‚Üí –≤—ã–≤–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö + –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–∞—è –ª–æ–≥–∏–∫–∞. –ë–µ–∑ –¥–æ–º—ã—Å–ª–æ–≤ –æ –±–∏–∑–Ω–µ—Å–µ
* Drill-down ‚Üí –ø–æ–ª–Ω–∞—è –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è, –≥–∏–ø–æ—Ç–µ–∑—ã, –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π

–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ ‚Äî —Ç–æ–ª—å–∫–æ –ø—Ä–∏ —Ç—Ä—ë—Ö —É—Å–ª–æ–≤–∏—è—Ö –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ:
1. –î–∞–Ω–Ω—ã–µ –¥–ª—è –Ω–µ—ë –µ—Å—Ç—å –≤ –≤—ã–≥—Ä—É–∑–∫–µ
2. –ö–∞–Ω–∞–ª/–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –≤–∏–¥–µ–Ω –≤ –¥–∞–Ω–Ω—ã—Ö (–Ω–µ –ø—Ä–µ–¥–ª–∞–≥–∞—Ç—å —Ç–æ, —á–µ–≥–æ –Ω–µ—Ç)
3. –î–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è ‚Äî –µ—Å—Ç—å CR –∏–ª–∏ spend –ø–æ —ç—Ç–æ–π —Å—É—â–Ω–æ—Å—Ç–∏

–ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ ‚Üí –ø—Ä–µ–¥–ª–æ–∂–∏ —Å–ª–µ–¥—É—é—â–∏–π —à–∞–≥: "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å CR —ç—Ç–æ–π –∫–∞–º–ø–∞–Ω–∏–∏?"

–ü—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: –µ—Å–ª–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–Ω –Ω–µ—Ç—Ä–∏–≤–∏–∞–ª—å–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω ‚Äî –Ω–∞–∑–æ–≤–∏ –µ–≥–æ, –¥–∞–∂–µ –µ—Å–ª–∏ –Ω–µ —Å–ø—Ä–æ—Å–∏–ª–∏. –û–¥–∏–Ω –∏–Ω—Å–∞–π—Ç —Å–≤–µ—Ä—Ö –≤–æ–ø—Ä–æ—Å–∞ ‚Äî –Ω–æ—Ä–º–∞. –î–≤–∞ –∏ –±–æ–ª—å—à–µ ‚Äî —É–∂–µ –±–∞–ª–ª–∞—Å—Ç.

–ö–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –≤ –≤—ã–≤–æ–¥–µ –¥–æ–ª–∂–Ω–æ –Ω–µ—Å—Ç–∏ —Å–º—ã—Å–ª. –ù–∏–∫–∞–∫–∏—Ö –∏—Ç–æ–≥–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤ —Å —ç–º–æ–¥–∑–∏, –ø–æ–≤—Ç–æ—Ä–æ–≤, –æ–±–æ–±—â–µ–Ω–∏–π —Ä–∞–¥–∏ –æ–±–æ–±—â–µ–Ω–∏–π.

## –ü—Ä–∞–≤–∏–ª–∞ Python-–∫–æ–¥–∞:
1. `df` —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω ‚Äî –ù–ï –≤—ã–∑—ã–≤–∞–π pd.read_parquet()
2. –í–°–ï–ì–î–ê —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–π `result` (Markdown —Å—Ç—Ä–æ–∫–∞ —Å –∏—Ç–æ–≥–æ–º)
3. –ò—Å–ø–æ–ª—å–∑—É–π print() –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —à–∞–≥–æ–≤: print("üìä –®–∞–≥ 1: ...")
4. –ü–æ–¥–ø–∏—Å—ã–≤–∞–π –≥—Ä–∞—Ñ–∏–∫–∏ –Ω–∞ –†–£–°–°–ö–û–ú: plt.title(), plt.xlabel(), plt.ylabel()
5. –§–æ—Ä–º–∞—Ç–∏—Ä—É–π —á–∏—Å–ª–∞: f"{value:,.0f}" (—Ü–µ–ª—ã–µ), f"{value:,.2f}" (–¥—Ä–æ–±–Ω—ã–µ)
6. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–π –ø—Ä–æ–ø—É—Å–∫–∏: df.dropna() –∏–ª–∏ df.fillna(0)
7. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞ ‚Äî plt.tight_layout() –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º
8. –ì—Ä–∞—Ñ–∏–∫ —Å—Ç—Ä–æ–π –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –æ–Ω —è–≤–Ω–æ –Ω—É–∂–µ–Ω –ø–æ —Ç–∏–ø—É –≤–æ–ø—Ä–æ—Å–∞ (—Å–º. —à–∞–≥ 4) ‚Äî –Ω–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

## –†–µ–∫–ª–∞–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
–§–æ—Ä–º—É–ª—ã –Ω–µ –Ω—É–∂–Ω–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—å –≤ –∫–∞–∂–¥–æ–º –æ—Ç–≤–µ—Ç–µ. –ù–æ:
* –ï—Å–ª–∏ —Å—á–∏—Ç–∞–µ—à—å –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –º–µ—Ç—Ä–∏–∫—É –∏–ª–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—É—é ‚Äî –ø–æ–∫–∞–∂–∏ —Ñ–æ—Ä–º—É–ª—É –æ–¥–∏–Ω —Ä–∞–∑
* –ï—Å–ª–∏ –≤–≤–æ–¥–∏—à—å –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É –∫–æ—Ç–æ—Ä—É—é –º–∞—Ä–∫–µ—Ç–æ–ª–æ–≥ –º–æ–≥ –Ω–µ –∑–Ω–∞—Ç—å ‚Äî —Ä–∞—Å—à–∏—Ñ—Ä—É–π –ø—Ä–∏ –ø–µ—Ä–≤–æ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏–∏
* –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ—Ç (–Ω–µ—Ç —Ä–∞—Å—Ö–æ–¥–∞ ‚Üí –Ω–µ—Ç CPC/CPA/ROAS) ‚Äî —Å–∫–∞–∂–∏ –ø—Ä—è–º–æ, –Ω–µ –¥–æ–¥—É–º—ã–≤–∞–π

## –°—Ç–∏–ª—å –æ—Ç–≤–µ—Ç–∞
* Markdown: –∑–∞–≥–æ–ª–æ–≤–∫–∏ ##/###, —Ç–∞–±–ª–∏—Ü—ã, –∂–∏—Ä–Ω—ã–π –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —Ü–∏—Ñ—Ä
* –≠–º–æ–¥–∑–∏ ‚Äî —Ç–æ–ª—å–∫–æ ‚ö†Ô∏è –¥–ª—è –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π. –ë–æ–ª—å—à–µ –Ω–∏–≥–¥–µ
* –ß–∏—Å–ª–∞ —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏ —Ç—ã—Å—è—á: 1 234 567
* –Ø–∑—ã–∫ ‚Äî —Ä—É—Å—Å–∫–∏–π
* –ö–æ–Ω–∫—Ä–µ—Ç–∏–∫–∞: —Ü–∏—Ñ—Ä—ã, –¥–∏–Ω–∞–º–∏–∫–∞, —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ ‚Äî –±–µ–∑ –≤–æ–¥—ã
"""


class AnalyticsAgent:
    """
    Wraps LangGraph ReAct agent with:
      - Claude Sonnet 4.6 via OpenRouter
      - SqliteSaver for session memory
      - Helper methods to extract plots and tool-call logs from agent output
    """

    def __init__(self) -> None:
        if not OPENROUTER_API_KEY:
            raise ValueError(
                "OPENROUTER_API_KEY is not set in .env. "
                "Get your key at https://openrouter.ai"
            )

        # ‚îÄ‚îÄ LLM via OpenRouter ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        self.llm = ChatOpenAI(
            model=MODEL,
            api_key=OPENROUTER_API_KEY,
            base_url="https://openrouter.ai/api/v1",
            max_tokens=MAX_TOKENS,
            default_headers={
                "HTTP-Referer": "https://server.asktab.ru",
                "X-Title": "ClickHouse Analytics Agent",
            },
        )

        # ‚îÄ‚îÄ SqliteSaver checkpointer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # Keeps conversation state per thread_id (= session_id).
        # Thread-safe for concurrent requests.
        conn = sqlite3.connect(str(DB_PATH), check_same_thread=False)
        self.memory = SqliteSaver(conn)


        # ‚îÄ‚îÄ LangGraph ReAct agent ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # _build_messages_for_llm is called before every LLM invocation:
        #   ‚Ä¢ prepends the system prompt (not stored in checkpoint)
        #   ‚Ä¢ compresses ToolMessages from previous turns to cut token usage
        self.graph = create_react_agent(
            model=self.llm,
            tools=TOOLS,
            prompt=_build_messages_for_llm,
            checkpointer=self.memory,
        )

        print(f"‚úÖ AnalyticsAgent ready | model: {MODEL} | db: {DB_PATH}")

    # ‚îÄ‚îÄ‚îÄ Public API ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def analyze(self, user_query: str, session_id: str) -> dict:
        """
        Process a user analytics query for a given session.

        Args:
            user_query: The user's question or request.
            session_id: Unique session identifier (= LangGraph thread_id).
                        Reuse the same session_id across requests to maintain context.

        Returns:
            {
              "success":    bool,
              "session_id": str,
              "text_output": str,         # Final Markdown text from the agent
              "plots":      list[str],    # base64 PNG data URIs from python_analysis
              "tool_calls": list[dict],   # Log of tool invocations
              "error":      str | None,
            }
        """
        config = {"configurable": {"thread_id": session_id}}

        try:
            # LangGraph invoke ‚Äî sends only the NEW message;
            # history is loaded automatically from SqliteSaver by thread_id.
            result = self.graph.invoke(
                {"messages": [HumanMessage(content=user_query)]},
                config=config,
            )

            messages: list = result.get("messages", [])

            text_output = self._extract_final_text(messages)
            plots = self._extract_plots(messages)
            tool_calls = self._extract_tool_calls(messages)

            return {
                "success": True,
                "session_id": session_id,
                "text_output": text_output,
                "plots": plots,
                "tool_calls": tool_calls,
                "error": None,
            }

        except Exception as exc:
            import traceback as tb
            return {
                "success": False,
                "session_id": session_id,
                "text_output": "",
                "plots": [],
                "tool_calls": [],
                "error": str(exc),
                "traceback": tb.format_exc(),
            }

    def get_session_info(self, session_id: str) -> dict:
        """Return basic metadata about a session."""
        try:
            config = {"configurable": {"thread_id": session_id}}
            state = self.graph.get_state(config)
            msgs = state.values.get("messages", []) if state and state.values else []
            # Count only user-visible exchanges (HumanMessage + AIMessage pairs)
            user_msgs = sum(1 for m in msgs if isinstance(m, HumanMessage))
            return {
                "session_id": session_id,
                "total_messages": len(msgs),
                "user_turns": user_msgs,
                "has_history": user_msgs > 0,
            }
        except Exception:
            return {
                "session_id": session_id,
                "total_messages": 0,
                "user_turns": 0,
                "has_history": False,
            }

    def cleanup_temp_files(self) -> int:
        """Delete Parquet files older than TEMP_FILE_TTL_SECONDS. Returns count deleted."""
        cutoff = time.time() - TEMP_FILE_TTL_SECONDS
        deleted = 0
        for f in TEMP_DIR.glob("*.parquet"):
            try:
                if f.stat().st_mtime < cutoff:
                    f.unlink()
                    deleted += 1
            except OSError:
                pass
        if deleted:
            print(f"üóëÔ∏è  Deleted {deleted} expired parquet file(s)")
        return deleted

    # ‚îÄ‚îÄ‚îÄ Private helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _extract_final_text(self, messages: list) -> str:
        """Return content of the last AIMessage that has non-empty text."""
        for msg in reversed(messages):
            if not isinstance(msg, AIMessage):
                continue
            content = msg.content
            if isinstance(content, str) and content.strip():
                return content
            # Some models return list of content blocks
            if isinstance(content, list):
                parts = [
                    block["text"]
                    for block in content
                    if isinstance(block, dict) and block.get("type") == "text"
                ]
                text = "\n".join(parts).strip()
                if text:
                    return text
        return ""

    def _extract_plots(self, messages: list) -> list[str]:
        """
        Extract base64 PNG plots from python_analysis ToolMessages
        that belong to the CURRENT agent run (after the last HumanMessage).

        Plots are stored in ToolMessage.artifact (not in .content) so they
        are never sent to the LLM, only kept in the checkpoint state for us.
        """
        # Find index of the most recently added HumanMessage
        last_human_idx = -1
        for i, msg in enumerate(messages):
            if isinstance(msg, HumanMessage):
                last_human_idx = i

        if last_human_idx < 0:
            return []

        plots: list[str] = []
        for msg in messages[last_human_idx:]:
            if not isinstance(msg, ToolMessage):
                continue
            tool_name = getattr(msg, "name", "") or ""
            if tool_name != "python_analysis":
                continue
            # Plots are in .artifact (base64 list), NOT in .content
            artifact = getattr(msg, "artifact", None)
            if isinstance(artifact, list):
                plots.extend(artifact)

        return plots

    def _extract_tool_calls(self, messages: list) -> list[dict]:
        """
        Extract a compact log of tool calls made during the current run.
        """
        last_human_idx = -1
        for i, msg in enumerate(messages):
            if isinstance(msg, HumanMessage):
                last_human_idx = i

        if last_human_idx < 0:
            return []

        tool_calls: list[dict] = []
        for msg in messages[last_human_idx:]:
            if not isinstance(msg, AIMessage):
                continue
            for tc in getattr(msg, "tool_calls", []):
                name = tc.get("name", "")
                args = tc.get("args", {})
                # Truncate large args for the log
                compact_args = {
                    k: (v[:300] + "‚Ä¶" if isinstance(v, str) and len(v) > 300 else v)
                    for k, v in args.items()
                }
                tool_calls.append({"tool": name, "input": compact_args})

        return tool_calls


# ‚îÄ‚îÄ‚îÄ Global singleton ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
_agent: Optional[AnalyticsAgent] = None


def get_agent() -> AnalyticsAgent:
    """Return (or create) the global AnalyticsAgent instance."""
    global _agent
    if _agent is None:
        _agent = AnalyticsAgent()
    return _agent
